# =============================================================================
# Adaptive Web Crawler - Local Ollama Configuration
# =============================================================================
# 
# This configuration uses a local Ollama instance for LLM descriptions.
# No API keys required - runs completely locally.
#
# Prerequisites:
#   1. Install Ollama: https://ollama.ai
#   2. Pull a model: ollama pull llama3.2
#   3. Start Ollama: ollama serve
#
# =============================================================================

crawl:
  seed_urls:
    - "https://example.com"
  output_dir: "./output"
  max_depth: 5
  max_pages: 500
  allowed_domains: []
  exclude_patterns:
    - "*.pdf"

identity:
  user_agent: "AdaptiveCrawler/1.0 (+https://example.com/bot)"

redis:
  url: "redis://localhost:6379/0"

rate_limit:
  default_delay: 1.5          # Slightly slower to account for local LLM
  max_concurrent_per_domain: 1
  max_concurrent_global: 5

# -----------------------------------------------------------------------------
# Local Ollama Structure Store
# -----------------------------------------------------------------------------
structure_store:
  store_type: "llm"
  enable_embeddings: true
  embedding_model: "all-MiniLM-L6-v2"
  ttl_seconds: 604800
  max_versions: 10

  llm:
    provider: "ollama"
    model: "llama3.2"         # Or: mistral, codellama, etc.
    ollama_base_url: "http://localhost:11434"

adaptive:
  enabled: true
  change_threshold: 0.3

safety:
  max_page_size_mb: 10.0
  request_timeout_seconds: 60.0  # Longer timeout for local LLM
  max_retries: 3

logging:
  level: "INFO"
  format: "json"
