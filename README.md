# Adaptive Web Crawler

An intelligent, compliance-first web crawler with ML-based structure learning that automatically adapts to website changes. Built for ethical, legal web data collection with full CFAA/GDPR/CCPA compliance.

## Table of Contents

- [Features](#features)
- [How It Works](#how-it-works)
- [Quick Start](#quick-start)
- [Architecture](#architecture)
- [Crawl Workflow](#crawl-workflow)
- [Adaptive Learning System](#adaptive-learning-system)
- [Sitemap Processing](#sitemap-processing)
- [JavaScript Rendering](#javascript-rendering)
- [Distributed Crawling](#distributed-crawling)
- [Scheduled Recrawling](#scheduled-recrawling)
- [Configuration](#configuration)
- [Usage Examples](#usage-examples)
- [Sports News Monitor Example](#sports-news-monitor-example)
- [API Reference](#api-reference)
- [Machine Learning Features](#machine-learning-features)
- [Development](#development)
- [Legal Notice](#legal-notice)

---

## Features

### Compliance & Legal

The crawler is designed with legal compliance as a first-class priority, ensuring your web scraping operations remain within legal boundaries.

#### CFAA Compliance (Computer Fraud and Abuse Act)

The CFAA is a U.S. federal law that prohibits unauthorized access to computer systems. The crawler implements authorization checks before every request:

```python
from crawler.legal import CFAAChecker

checker = CFAAChecker()

# Check if crawling is authorized
result = await checker.is_authorized("https://example.com/page")
if result.authorized:
    print("Crawling is authorized")
else:
    print(f"Blocked: {result.reason}")
    # Possible reasons:
    # - "Terms of service explicitly prohibit crawling"
    # - "Login-required content without authorization"
    # - "Previously received cease-and-desist"
```

**Authorization indicators the crawler checks:**
- Public accessibility (no authentication required)
- Presence of robots.txt (indicates expectation of bots)
- Meta tags allowing/disallowing indexing
- **Terms of Service analysis (enabled by default)** - automatically analyzes ToS for crawling restrictions
- Previous crawl history and any blocks received

**Terms of Service Analysis:**

The crawler **automatically analyzes Terms of Service** pages to detect crawling restrictions. This feature is **enabled by default** to ensure maximum legal compliance.

```python
from crawler.config import CFAAConfig
from crawler.legal import CFAAChecker

# ToS analysis is enabled by default
cfaa_config = CFAAConfig(
    enabled=True,
    tos_analysis_enabled=True,        # Enabled by default
    block_on_restrictive_tos=True,    # Block crawling if ToS prohibits it
    tos_cache_ttl=86400,              # Cache ToS analysis for 24 hours
    common_tos_paths=[                # Paths to check for ToS
        "/terms",
        "/terms-of-service",
        "/tos",
        "/legal/terms",
        "/terms-and-conditions",
        "/terms-of-use",
    ],
)

# The checker automatically analyzes ToS
checker = CFAAChecker(config=cfaa_config)

# When checking authorization, ToS is analyzed automatically
result = await checker.is_authorized(url)
if not result.authorized and result.basis == "terms_of_service":
    print(f"ToS prohibits crawling: {result.documentation}")

# You can also analyze ToS text directly
tos_analysis = checker.analyze_tos(tos_text, domain)
print(f"Restrictive: {tos_analysis['is_restrictive']}")
print(f"Restrictions: {tos_analysis['restrictions']}")
```

**What ToS analysis detects:**
- Explicit prohibition of scraping/crawling
- Prohibition of automated access
- Requirements to use official APIs only
- Rate limit mentions
- Bot/spider restrictions

#### GDPR/CCPA Support

The General Data Protection Regulation (GDPR) and California Consumer Privacy Act (CCPA) require special handling of personal data. The crawler provides:

```python
from crawler.legal import PIIDetector, PIIHandler
from crawler.config import GDPRConfig, PIIHandlingConfig

# Configure GDPR compliance
gdpr_config = GDPRConfig(
    enabled=True,
    retention_days=365,           # Auto-delete data after 1 year
    collect_only=["url", "title", "content"],  # Whitelist fields
    exclude_countries=["EU"],     # Optional: skip EU-based sites
)

# Configure PII handling
pii_config = PIIHandlingConfig(
    action="redact",              # Options: "redact", "pseudonymize", "exclude_page"
    patterns=[
        r"\b\d{3}-\d{2}-\d{4}\b",  # SSN pattern
        r"\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b",  # Email
        r"\b\d{16}\b",             # Credit card
    ],
    log_detections=True,          # Audit trail for compliance
)

# Detect PII in content
detector = PIIDetector()
findings = detector.scan(html_content)
for finding in findings:
    print(f"Found {finding.pii_type} at position {finding.start}-{finding.end}")
    # Output: Found EMAIL at position 1234-1256

# Handle PII according to policy
handler = PIIHandler(pii_config)
clean_content = handler.process(html_content)
```

#### robots.txt Respect (RFC 9309)

The crawler fully implements the robots.txt standard including the latest RFC 9309 specification:

```python
from crawler.compliance import RobotsChecker

checker = RobotsChecker(
    user_agent="MyCrawler/1.0",
    cache_ttl=3600,  # Cache robots.txt for 1 hour
)

# Check if path is allowed
allowed = await checker.is_allowed("https://example.com/private/page")
print(f"Allowed: {allowed}")

# Get crawl delay
delay = await checker.get_crawl_delay("https://example.com")
print(f"Crawl-delay: {delay} seconds")

# Get sitemap URLs from robots.txt
sitemaps = await checker.get_sitemaps("https://example.com")
print(f"Sitemaps: {sitemaps}")
```

**Supported robots.txt directives:**
- `User-agent` - Matches crawler identity
- `Allow` / `Disallow` - Path-based access control
- `Crawl-delay` - Per-domain rate limiting
- `Sitemap` - Sitemap discovery
- Wildcard patterns (`*`, `$`)

#### Anti-Bot Respect

Unlike aggressive scrapers, this crawler treats bot detection as "access denied" and never attempts evasion:

```python
# The crawler automatically detects and respects:
# - CAPTCHA challenges â†’ marks URL as blocked
# - JavaScript challenges (Cloudflare, etc.) â†’ marks as blocked
# - Rate limit responses (429) â†’ backs off exponentially
# - IP blocks â†’ stops crawling that domain

# You can check if a domain has blocked the crawler:
from crawler.compliance import BlockedDomainTracker

tracker = BlockedDomainTracker(redis_client)
if await tracker.is_blocked("example.com"):
    print("Domain has blocked our crawler")
    print(f"Blocked since: {await tracker.get_block_time('example.com')}")
    print(f"Reason: {await tracker.get_block_reason('example.com')}")
```

### Intelligent Crawling

#### Adaptive Rate Limiting

The crawler automatically adjusts request rates based on server responses:

```python
from crawler.compliance import AdaptiveRateLimiter

limiter = AdaptiveRateLimiter(
    default_delay=1.0,    # Start with 1 second between requests
    min_delay=0.5,        # Never go faster than 0.5 seconds
    max_delay=60.0,       # Never wait more than 60 seconds
    backoff_factor=2.0,   # Double delay on rate limit
    recovery_factor=0.9,  # Slowly recover after success
)

# The limiter automatically tracks per-domain delays
async with limiter.acquire("example.com"):
    # Make request here
    response = await fetch(url)

    # Report response for adaptive adjustment
    if response.status_code == 429:
        limiter.report_rate_limited("example.com")
        # Delay automatically increases
    elif response.status_code == 503:
        limiter.report_server_overload("example.com")
        # Delay automatically increases
    else:
        limiter.report_success("example.com")
        # Delay slowly decreases toward default
```

#### Structure Learning

The ML-based DOM analysis learns page layouts automatically:

```python
from crawler.adaptive import StructureAnalyzer, StructureLearner

analyzer = StructureAnalyzer()

# Analyze page structure
structure = analyzer.analyze(
    html=html_content,
    url="https://example.com/article/123",
    page_type="article",
)

# The structure contains:
print(f"Domain: {structure.domain}")
print(f"Tag hierarchy: {structure.tag_hierarchy}")
print(f"CSS classes: {structure.css_class_map}")
print(f"Element IDs: {structure.id_attributes}")
print(f"Semantic landmarks: {structure.semantic_landmarks}")
print(f"Content regions: {structure.content_regions}")
print(f"Navigation selectors: {structure.navigation_selectors}")

# Learn extraction strategy
learner = StructureLearner()
strategy = learner.infer(html_content, structure)

print(f"Title selector: {strategy.title.selector} (confidence: {strategy.title.confidence})")
print(f"Content selector: {strategy.content.selector} (confidence: {strategy.content.confidence})")
```

#### Change Detection

Automatically detects when websites change their structure:

```python
from crawler.adaptive import ChangeDetector, ChangeClassification

detector = ChangeDetector()

# Compare old and new structures
analysis = detector.detect_changes(old_structure, new_structure)

print(f"Has changes: {analysis.has_changes}")
print(f"Similarity: {analysis.similarity_score:.2%}")
print(f"Classification: {analysis.classification.name}")

# Classification levels:
# COSMETIC (â‰¥95%): CSS-only changes, no action needed
# MINOR (85-95%): Small tweaks, keep strategy
# MODERATE (70-85%): Significant changes, consider adapting
# BREAKING (<70%): Major redesign, must re-learn strategy

# Get detailed change information
for change in analysis.changes:
    print(f"  - {change.change_type}: {change.description}")
    # Examples:
    # - TAG_COUNT_CHANGED: div count changed from 45 to 52
    # - CLASS_RENAMED: .article-content â†’ .post-body
    # - ELEMENT_MOVED: #sidebar moved from right to left
    # - LANDMARK_ADDED: New <aside> element detected
```

### Production Ready

#### Redis-Backed Persistence

All learned structures and strategies are stored in Redis for persistence:

```python
from crawler.storage import StructureStore

store = StructureStore(redis_url="redis://localhost:6379/0")

# Save structure
await store.save_structure("example.com", "article", structure)

# Load structure
stored = await store.get_structure("example.com", "article")

# Get structure history (for rollback)
history = await store.get_history("example.com", "article", limit=10)
for version in history:
    print(f"Version {version.version} at {version.timestamp}")

# Rollback to previous version
await store.rollback("example.com", "article", version=3)

# Structure TTL and expiration
await store.set_ttl("example.com", "article", seconds=604800)  # 7 days
```

#### Structured Logging

Complete audit trail of all operations:

```python
import structlog
from crawler.utils import configure_logging

# Configure structured logging
configure_logging(
    level="INFO",
    format="json",  # or "console" for development
    output="crawler.log",
)

log = structlog.get_logger()

# All crawler operations are logged with context
log.info("page_crawled",
    url="https://example.com/page",
    status_code=200,
    content_length=15234,
    extraction_success=True,
    selectors_used=["h1.title", "article.content"],
)

# Compliance events are logged for audit
log.info("robots_check",
    url="https://example.com/private",
    allowed=False,
    reason="Disallow: /private",
)

log.info("pii_detected",
    url="https://example.com/page",
    pii_type="EMAIL",
    action="redacted",
    count=3,
)
```

#### Circuit Breakers

Automatic failure isolation per domain prevents cascading failures:

```python
from crawler.utils import CircuitBreaker

breaker = CircuitBreaker(
    failure_threshold=5,     # Open after 5 failures
    recovery_timeout=60,     # Try again after 60 seconds
    half_open_requests=3,    # Allow 3 test requests when half-open
)

async def fetch_with_circuit_breaker(url: str):
    domain = get_domain(url)

    if breaker.is_open(domain):
        raise CircuitOpenError(f"Circuit open for {domain}")

    try:
        response = await fetch(url)
        breaker.record_success(domain)
        return response
    except Exception as e:
        breaker.record_failure(domain)
        raise

# Check circuit status
status = breaker.get_status("example.com")
print(f"State: {status.state}")  # CLOSED, OPEN, or HALF_OPEN
print(f"Failures: {status.failure_count}")
print(f"Last failure: {status.last_failure_time}")
```

#### Parallel Crawling

Configurable concurrency with domain politeness:

```python
from crawler.core import ConcurrencyManager

manager = ConcurrencyManager(
    global_limit=50,          # Max 50 concurrent requests total
    per_domain_limit=5,       # Max 5 concurrent requests per domain
    per_ip_limit=10,          # Max 10 concurrent requests per IP
)

async def crawl_with_limits(urls: list[str]):
    async with manager:
        tasks = []
        for url in urls:
            # This automatically respects all limits
            task = manager.submit(fetch, url)
            tasks.append(task)

        results = await asyncio.gather(*tasks)

    return results

# Monitor concurrency
stats = manager.get_stats()
print(f"Active requests: {stats.active_count}")
print(f"Queued requests: {stats.queued_count}")
print(f"Domains active: {stats.domains_active}")
```

### Advanced Crawling
- **Sitemap Processing**: Full XML sitemap support with recursive index handling and gzip decompression
- **JavaScript Rendering**: Playwright integration for SPAs with automatic JS requirement detection
- **Distributed Crawling**: Multi-worker coordination with Redis queues, heartbeats, and leader election
- **Scheduled Recrawling**: Cron-like scheduling with adaptive intervals based on content change frequency

---

## How It Works

### High-Level Flow

```
                                    ADAPTIVE WEB CRAWLER
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                                                                             â”‚
    â”‚   SEED URLs â”€â”€â–º SCHEDULER â”€â”€â–º FETCHER â”€â”€â–º ANALYZER â”€â”€â–º EXTRACTOR â”€â”€â–º STORAGEâ”‚
    â”‚                     â”‚            â”‚           â”‚             â”‚                â”‚
    â”‚                     â”‚            â”‚           â”‚             â”‚                â”‚
    â”‚                     â–¼            â–¼           â–¼             â–¼                â”‚
    â”‚               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
    â”‚               â”‚  URL    â”‚  â”‚Complianceâ”‚ â”‚Structureâ”‚  â”‚ Learned  â”‚          â”‚
    â”‚               â”‚Frontier â”‚  â”‚ Pipeline â”‚ â”‚Learning â”‚  â”‚ Strategy â”‚          â”‚
    â”‚               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
    â”‚                                                                             â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### The Compliance Pipeline

Every URL request passes through a strict compliance pipeline:

```
    URL Request
         â”‚
         â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ 1. CFAA Check      â”‚ â—„â”€â”€ Is crawling this URL legally authorized?
    â”‚    (Authorization) â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚ Authorized
             â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ 2. robots.txt      â”‚ â—„â”€â”€ Does the site allow crawling this path?
    â”‚    Check           â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚ Allowed
             â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ 3. Rate Limiter    â”‚ â—„â”€â”€ Wait for appropriate delay (respects Crawl-delay)
    â”‚    (Per-Domain)    â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚ Ready
             â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ 4. HTTP Fetch      â”‚ â—„â”€â”€ Actual request with timeout & retries
    â”‚                    â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚ Response
             â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ 5. GDPR/PII Check  â”‚ â—„â”€â”€ Detect and handle personal data
    â”‚    (if enabled)    â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â–¼
       FetchResult
```

---

## Quick Start

### Prerequisites

| Requirement | Version | Purpose |
|-------------|---------|---------|
| Python | 3.11+ | Runtime environment |
| Redis | 7.0+ | Structure persistence, rate limiting, distributed features |
| Docker (optional) | 20.0+ | Easiest way to run Redis |

**Verify Python version:**
```bash
python --version
# Should output: Python 3.11.x or higher
```

### Installation

#### Step 1: Clone and Setup Environment

```bash
# Clone the repository
git clone https://github.com/yourusername/adaptive-crawler.git
cd adaptive-crawler

# Create virtual environment
python -m venv .venv

# Activate virtual environment
# On Linux/macOS:
source .venv/bin/activate
# On Windows (Command Prompt):
.venv\Scripts\activate.bat
# On Windows (PowerShell):
.venv\Scripts\Activate.ps1

# Verify activation (should show path to .venv)
which python  # Linux/macOS
where python  # Windows
```

#### Step 2: Install Dependencies

```bash
# Basic installation
pip install -e .

# With development dependencies (testing, linting)
pip install -e ".[dev]"

# With ML features (embeddings, classification)
pip install -e ".[ml]"

# With LLM support (OpenAI, Anthropic, Ollama)
pip install -e ".[llm]"

# With JavaScript rendering (Playwright)
pip install -e ".[js-rendering]"
playwright install chromium

# Everything
pip install -e ".[dev,ml,llm,js-rendering]"
```

#### Step 3: Verify Installation

```bash
# Check the crawler is installed
python -c "import crawler; print(f'Crawler version: {crawler.__version__}')"

# Run module check
python -m crawler --help
```

**Expected output:**
```
usage: crawler [-h] --seed-url URL [--output DIR] [--max-depth N]
               [--max-pages N] [--rate-limit SECONDS] ...

Adaptive Web Crawler - Intelligent, compliance-first web crawling
```

### Start Redis

Redis is required for the adaptive features (structure learning, change detection, rate limiting).

#### Option 1: Docker (Recommended)

```bash
# Start Redis container
docker run -d --name redis-crawler -p 6379:6379 redis:7-alpine

# Verify it's running
docker ps | grep redis-crawler

# Check Redis is responding
docker exec redis-crawler redis-cli ping
# Should output: PONG

# View logs if needed
docker logs redis-crawler
```

#### Option 2: Docker Compose

Create `docker-compose.yml`:
```yaml
version: '3.8'
services:
  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    command: redis-server --appendonly yes
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 5

volumes:
  redis_data:
```

```bash
docker-compose up -d
docker-compose ps  # Verify status
```

#### Option 3: Local Installation

**Debian/Ubuntu:**
```bash
curl -fsSL https://packages.redis.io/gpg | sudo gpg --dearmor -o /usr/share/keyrings/redis-archive-keyring.gpg
echo "deb [signed-by=/usr/share/keyrings/redis-archive-keyring.gpg] https://packages.redis.io/deb $(lsb_release -cs) main" | sudo tee /etc/apt/sources.list.d/redis.list
sudo apt-get update
sudo apt-get install -y redis-server

# Start Redis
sudo systemctl start redis-server
sudo systemctl enable redis-server

# Verify
redis-cli ping
```

**macOS (Homebrew):**
```bash
brew install redis
brew services start redis
redis-cli ping
```

**Windows:**
```powershell
# Using Windows Subsystem for Linux (WSL) is recommended
# Or use Docker Desktop for Windows
```

#### Verify Redis Connection

```bash
# Test connection from Python
python -c "
import redis
r = redis.Redis(host='localhost', port=6379, db=0)
print(f'Redis connected: {r.ping()}')
print(f'Redis version: {r.info()[\"redis_version\"]}')
"
```

### Run Your First Crawl

#### Basic Crawl

```bash
# Minimal crawl
python -m crawler --seed-url https://example.com --output ./data

# Expected output:
# [INFO] Starting crawl with 1 seed URL(s)
# [INFO] Crawling: https://example.com
# [INFO] Fetched: https://example.com (200 OK, 1.2KB)
# [INFO] Structure learned for example.com/homepage
# [INFO] Extracted: title, content
# [INFO] Crawl complete: 1 pages, 0 errors
```

#### Crawl with Options

```bash
python -m crawler \
    --seed-url https://example.com \
    --output ./data \
    --max-depth 5 \
    --max-pages 100 \
    --rate-limit 0.5 \
    --user-agent "MyCrawler/1.0 (+https://mysite.com/bot)" \
    --respect-robots \
    --verbose
```

#### Verify Output

```bash
# Check output directory structure
ls -la ./data/
# Expected:
# data/
# â”œâ”€â”€ raw/                    # Raw HTML files
# â”‚   â””â”€â”€ example.com/
# â”‚       â””â”€â”€ index.html
# â”œâ”€â”€ extracted/              # Extracted JSON data
# â”‚   â””â”€â”€ example.com/
# â”‚       â””â”€â”€ index.json
# â”œâ”€â”€ metadata/               # Crawl metadata
# â”‚   â””â”€â”€ crawl_stats.json
# â””â”€â”€ logs/                   # Crawl logs
#     â””â”€â”€ crawler.log

# View extracted content
cat ./data/extracted/example.com/index.json
```

### Troubleshooting

#### Common Issues

**1. "Redis connection refused"**
```bash
# Check if Redis is running
redis-cli ping

# If not, start it
docker start redis-crawler  # If using Docker
sudo systemctl start redis-server  # If local install

# Check Redis logs
docker logs redis-crawler
# or
sudo journalctl -u redis-server
```

**2. "Module not found: crawler"**
```bash
# Make sure you're in the virtual environment
source .venv/bin/activate

# Reinstall the package
pip install -e .
```

**3. "Permission denied" when writing output**
```bash
# Check directory permissions
ls -la ./data/

# Create directory with proper permissions
mkdir -p ./data && chmod 755 ./data
```

**4. "SSL certificate verify failed"**
```bash
# Update certificates
pip install --upgrade certifi

# Or disable SSL verification (not recommended for production)
python -m crawler --seed-url https://example.com --no-verify-ssl
```

**5. "Rate limited (429)" errors**
```bash
# Increase delay between requests
python -m crawler --seed-url https://example.com --rate-limit 2.0

# The crawler will automatically back off, but you can start slower
```

**6. "Playwright not found" for JS rendering**
```bash
# Install Playwright and browsers
pip install playwright
playwright install chromium

# Verify installation
python -c "from playwright.sync_api import sync_playwright; print('Playwright OK')"
```

#### Debug Mode

```bash
# Run with maximum verbosity
python -m crawler \
    --seed-url https://example.com \
    --output ./data \
    --log-level DEBUG \
    --log-file ./crawler-debug.log

# View real-time logs
tail -f ./crawler-debug.log
```

#### Health Check Script

Create `check_setup.py`:
```python
#!/usr/bin/env python
"""Verify crawler setup is complete."""
import sys

def check_python():
    version = sys.version_info
    if version.major < 3 or (version.major == 3 and version.minor < 11):
        print(f"âŒ Python 3.11+ required (found {version.major}.{version.minor})")
        return False
    print(f"âœ… Python {version.major}.{version.minor}.{version.micro}")
    return True

def check_redis():
    try:
        import redis
        r = redis.Redis()
        r.ping()
        print("âœ… Redis connected")
        return True
    except Exception as e:
        print(f"âŒ Redis: {e}")
        return False

def check_crawler():
    try:
        import crawler
        print(f"âœ… Crawler installed (v{crawler.__version__})")
        return True
    except ImportError as e:
        print(f"âŒ Crawler: {e}")
        return False

def check_optional():
    results = []

    # Check Playwright
    try:
        from playwright.sync_api import sync_playwright
        results.append("âœ… Playwright (JS rendering)")
    except ImportError:
        results.append("âš ï¸  Playwright not installed (optional)")

    # Check ML dependencies
    try:
        import sentence_transformers
        results.append("âœ… sentence-transformers (ML features)")
    except ImportError:
        results.append("âš ï¸  sentence-transformers not installed (optional)")

    for r in results:
        print(r)

if __name__ == "__main__":
    print("Checking Adaptive Crawler setup...\n")

    all_ok = all([
        check_python(),
        check_redis(),
        check_crawler(),
    ])

    print()
    check_optional()

    print()
    if all_ok:
        print("ğŸ‰ All required components are ready!")
        sys.exit(0)
    else:
        print("âŒ Some required components are missing.")
        sys.exit(1)
```

Run the health check:
```bash
python check_setup.py
```

---

## Architecture

### Directory Structure

```
crawler/
â”œâ”€â”€ core/                    # Core orchestration
â”‚   â”œâ”€â”€ crawler.py          # Main crawler orchestrator
â”‚   â”œâ”€â”€ fetcher.py          # HTTP client + compliance pipeline
â”‚   â”œâ”€â”€ scheduler.py        # URL frontier management
â”‚   â”œâ”€â”€ renderer.py         # Playwright JS rendering
â”‚   â”œâ”€â”€ distributed.py      # Multi-worker coordination
â”‚   â””â”€â”€ recrawl_scheduler.py # Scheduled recrawling
â”‚
â”œâ”€â”€ compliance/             # Legal compliance
â”‚   â”œâ”€â”€ robots_parser.py    # RFC 9309 robots.txt parsing
â”‚   â”œâ”€â”€ rate_limiter.py     # Adaptive per-domain rate limiting
â”‚   â””â”€â”€ sitemap_parser.py   # XML sitemap parsing
â”‚
â”œâ”€â”€ legal/                  # Legal frameworks
â”‚   â”œâ”€â”€ cfaa_checker.py     # CFAA authorization checks
â”‚   â””â”€â”€ pii_detector.py     # GDPR/CCPA PII handling
â”‚
â”œâ”€â”€ extraction/             # Content extraction
â”‚   â”œâ”€â”€ link_extractor.py   # URL discovery
â”‚   â””â”€â”€ content_extractor.py # CSS selector-based extraction
â”‚
â”œâ”€â”€ adaptive/               # ML-based adaptation
â”‚   â”œâ”€â”€ structure_analyzer.py   # DOM fingerprinting
â”‚   â”œâ”€â”€ change_detector.py      # Structure comparison
â”‚   â””â”€â”€ strategy_learner.py     # CSS selector inference
â”‚
â”œâ”€â”€ storage/                # Persistence
â”‚   â”œâ”€â”€ url_store.py        # Visited URL tracking
â”‚   â”œâ”€â”€ robots_cache.py     # robots.txt caching
â”‚   â””â”€â”€ structure_store.py  # Learned structures (Redis)
â”‚
â””â”€â”€ utils/                  # Utilities
    â”œâ”€â”€ logging.py          # Structured logging
    â””â”€â”€ metrics.py          # Statistics tracking
```

### Component Interaction

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                           CRAWLER                                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”‚
â”‚  â”‚  Scheduler  â”‚â—„â”€â”€â–ºâ”‚   Fetcher   â”‚â—„â”€â”€â–ºâ”‚  Extractor  â”‚             â”‚
â”‚  â”‚             â”‚    â”‚             â”‚    â”‚             â”‚             â”‚
â”‚  â”‚ â€¢ URL Queue â”‚    â”‚ â€¢ Complianceâ”‚    â”‚ â€¢ Links     â”‚             â”‚
â”‚  â”‚ â€¢ Prioritiesâ”‚    â”‚ â€¢ HTTP      â”‚    â”‚ â€¢ Content   â”‚             â”‚
â”‚  â”‚ â€¢ Dedup     â”‚    â”‚ â€¢ Retries   â”‚    â”‚ â€¢ Metadata  â”‚             â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜             â”‚
â”‚         â”‚                  â”‚                  â”‚                     â”‚
â”‚         â”‚                  â–¼                  â”‚                     â”‚
â”‚         â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚                     â”‚
â”‚         â”‚         â”‚    ADAPTIVE     â”‚         â”‚                     â”‚
â”‚         â”‚         â”‚    SYSTEM       â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”˜                     â”‚
â”‚         â”‚         â”‚                 â”‚                               â”‚
â”‚         â”‚         â”‚ â€¢ Analyzer      â”‚                               â”‚
â”‚         â”‚         â”‚ â€¢ Detector      â”‚                               â”‚
â”‚         â”‚         â”‚ â€¢ Learner       â”‚                               â”‚
â”‚         â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                               â”‚
â”‚         â”‚                  â”‚                                        â”‚
â”‚         â–¼                  â–¼                                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚
â”‚  â”‚                    STORAGE                       â”‚               â”‚
â”‚  â”‚                                                  â”‚               â”‚
â”‚  â”‚  Redis: Structures, Strategies, URLs, Robots    â”‚               â”‚
â”‚  â”‚  Disk:  HTML, JSON, Extracted Content           â”‚               â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Crawl Workflow

### Complete Crawl Cycle

```
START
  â”‚
  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. INITIALIZE                       â”‚
â”‚    â€¢ Connect to Redis               â”‚
â”‚    â€¢ Load configuration             â”‚
â”‚    â€¢ Create output directory        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 2. ADD SEED URLs TO FRONTIER        â”‚
â”‚    â€¢ Validate URLs                  â”‚
â”‚    â€¢ Check allowed domains          â”‚
â”‚    â€¢ Initialize depth = 0           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
               â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  MAIN LOOP   â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                                  â”‚
               â”‚                                          â”‚
               â–¼                                          â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”‚
â”‚ 3. GET NEXT URL FROM SCHEDULER      â”‚                  â”‚
â”‚    â€¢ Priority: breadth-first        â”‚                  â”‚
â”‚    â€¢ Respect domain politeness      â”‚                  â”‚
â”‚    â€¢ Check max_depth, max_pages     â”‚                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚
               â”‚                                          â”‚
               â–¼                                          â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”‚
â”‚ 4. FETCH URL (Compliance Pipeline)  â”‚                  â”‚
â”‚    â€¢ CFAA check                     â”‚                  â”‚
â”‚    â€¢ robots.txt check               â”‚                  â”‚
â”‚    â€¢ Rate limit wait                â”‚                  â”‚
â”‚    â€¢ HTTP GET with timeout          â”‚                  â”‚
â”‚    â€¢ GDPR/PII processing            â”‚                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚
               â”‚                                          â”‚
               â–¼                                          â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                  â”‚
        â”‚   SUCCESS?   â”‚                                  â”‚
        â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                                  â”‚
               â”‚                                          â”‚
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”                                 â”‚
      â”‚                 â”‚                                 â”‚
      â–¼                 â–¼                                 â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                        â”‚
â”‚ BLOCKED/ â”‚     â”‚   SUCCESS    â”‚                        â”‚
â”‚ ERROR    â”‚     â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                        â”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜            â”‚                                â”‚
     â”‚                  â–¼                                â”‚
     â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
     â”‚     â”‚ 5. SAVE RAW CONTENT                 â”‚      â”‚
     â”‚     â”‚    â€¢ HTML to disk                   â”‚      â”‚
     â”‚     â”‚    â€¢ Metadata (headers, status)     â”‚      â”‚
     â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
     â”‚                    â”‚                              â”‚
     â”‚                    â–¼                              â”‚
     â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
     â”‚     â”‚ 6. EXTRACT & QUEUE LINKS            â”‚      â”‚
     â”‚     â”‚    â€¢ Parse <a href>                 â”‚      â”‚
     â”‚     â”‚    â€¢ Normalize URLs                 â”‚      â”‚
     â”‚     â”‚    â€¢ Add to scheduler               â”‚      â”‚
     â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
     â”‚                    â”‚                              â”‚
     â”‚                    â–¼                              â”‚
     â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
     â”‚     â”‚ 7. ADAPTIVE ANALYSIS                â”‚      â”‚
     â”‚     â”‚    â€¢ Analyze current structure      â”‚      â”‚
     â”‚     â”‚    â€¢ Compare with stored            â”‚      â”‚
     â”‚     â”‚    â€¢ Detect changes                 â”‚      â”‚
     â”‚     â”‚    â€¢ Adapt strategy if needed       â”‚      â”‚
     â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
     â”‚                    â”‚                              â”‚
     â”‚                    â–¼                              â”‚
     â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
     â”‚     â”‚ 8. EXTRACT CONTENT                  â”‚      â”‚
     â”‚     â”‚    â€¢ Apply learned CSS selectors    â”‚      â”‚
     â”‚     â”‚    â€¢ Extract title, content, meta   â”‚      â”‚
     â”‚     â”‚    â€¢ Save extracted JSON            â”‚      â”‚
     â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
     â”‚                    â”‚                              â”‚
     â–¼                    â–¼                              â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”‚
â”‚ 9. UPDATE STATISTICS                â”‚                 â”‚
â”‚    â€¢ Increment counters             â”‚                 â”‚
â”‚    â€¢ Log progress                   â”‚                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â”‚
               â”‚                                         â”‚
               â–¼                                         â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                â”‚
        â”‚ MORE URLs?   â”‚â”€â”€â”€â”€â”€â”€ YES â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚ NO
               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 10. FINALIZE                        â”‚
â”‚     â€¢ Close connections             â”‚
â”‚     â€¢ Return CrawlerStats           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
               â–¼
             END
```

---

## Adaptive Learning System

The adaptive system learns how to extract content from websites and automatically adjusts when sites change.

### Structure Analysis

The `StructureAnalyzer` creates a fingerprint of each page's DOM:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    PAGE STRUCTURE FINGERPRINT                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                  â”‚
â”‚  Tag Hierarchy          CSS Classes           Element IDs        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ div: 45      â”‚      â”‚ .article: 12 â”‚      â”‚ #header      â”‚  â”‚
â”‚  â”‚ span: 23     â”‚      â”‚ .nav-item: 8 â”‚      â”‚ #content     â”‚  â”‚
â”‚  â”‚ a: 67        â”‚      â”‚ .btn: 15     â”‚      â”‚ #footer      â”‚  â”‚
â”‚  â”‚ p: 12        â”‚      â”‚ .card: 6     â”‚      â”‚ #sidebar     â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                  â”‚
â”‚  Semantic Landmarks     Navigation            Content Regions    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ <article>    â”‚      â”‚ nav.main-nav â”‚      â”‚ .post-body   â”‚  â”‚
â”‚  â”‚ <nav>        â”‚      â”‚ ul.menu      â”‚      â”‚ article      â”‚  â”‚
â”‚  â”‚ <header>     â”‚      â”‚ .breadcrumb  â”‚      â”‚ .content     â”‚  â”‚
â”‚  â”‚ <footer>     â”‚      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                               â”‚
â”‚                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Change Detection

The `ChangeDetector` compares structures using weighted similarity:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    SIMILARITY CALCULATION                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                  â”‚
â”‚  Component              Weight    Example Similarity             â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€          â”‚
â”‚  Tag Hierarchy          30%       0.95 (minor changes)          â”‚
â”‚  Content Regions        25%       0.90 (same regions)           â”‚
â”‚  Navigation             15%       1.00 (unchanged)              â”‚
â”‚  Semantic Landmarks     15%       0.85 (added footer)           â”‚
â”‚  CSS Classes            10%       0.75 (renamed some)           â”‚
â”‚  Element IDs             5%       1.00 (unchanged)              â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€          â”‚
â”‚  WEIGHTED TOTAL                   0.92 (MINOR change)           â”‚
â”‚                                                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                    CLASSIFICATION THRESHOLDS                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                  â”‚
â”‚  â‰¥ 95%  COSMETIC   â”€â”€â–º  CSS-only changes, keep strategy         â”‚
â”‚  85-95% MINOR      â”€â”€â–º  Small tweaks, keep strategy             â”‚
â”‚  70-85% MODERATE   â”€â”€â–º  Significant changes, may adapt          â”‚
â”‚  < 70%  BREAKING   â”€â”€â–º  Major redesign, re-learn strategy       â”‚
â”‚                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Strategy Learning

The `StrategyLearner` infers CSS selectors for content extraction:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    SELECTOR INFERENCE                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                  â”‚
â”‚  For each field (title, content, date, author):                 â”‚
â”‚                                                                  â”‚
â”‚  1. Try patterns in order of confidence:                        â”‚
â”‚                                                                  â”‚
â”‚     TITLE PATTERNS                 CONTENT PATTERNS             â”‚
â”‚     â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€             â”‚
â”‚     h1.title        (0.90)         article        (0.90)        â”‚
â”‚     h1.entry-title  (0.90)         main           (0.85)        â”‚
â”‚     h1.post-title   (0.90)         .article-content (0.80)      â”‚
â”‚     article h1      (0.85)         .post-content  (0.80)        â”‚
â”‚     .article-title  (0.80)         .content       (0.70)        â”‚
â”‚     h1              (0.70)         body           (0.75) â—„â”€ fallback
â”‚     title           (0.75) â—„â”€ fallback                          â”‚
â”‚                                                                  â”‚
â”‚  2. Adjust confidence based on matches:                         â”‚
â”‚     â€¢ 1 element:   keep base confidence                         â”‚
â”‚     â€¢ 2-3 elements: Ã— 0.9                                       â”‚
â”‚     â€¢ 4+ elements:  Ã— 0.7                                       â”‚
â”‚                                                                  â”‚
â”‚  3. Accept if confidence â‰¥ min_confidence (0.5)                 â”‚
â”‚                                                                  â”‚
â”‚  4. Build ExtractionStrategy with selected rules                â”‚
â”‚                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Adaptation Flow

```
                    FIRST VISIT                    SUBSEQUENT VISITS
                    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                         â”‚                               â”‚
                         â–¼                               â–¼
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚ Analyze Structure   â”‚      â”‚ Analyze Structure   â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚                            â”‚
                         â–¼                            â–¼
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚ Infer Strategy      â”‚      â”‚ Load Stored         â”‚
              â”‚ (pattern matching)  â”‚      â”‚ Structure + Strategyâ”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚                            â”‚
                         â–¼                            â–¼
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚ Save to Redis       â”‚      â”‚ Compare Structures  â”‚
              â”‚ â€¢ Structure         â”‚      â”‚ (similarity score)  â”‚
              â”‚ â€¢ Strategy          â”‚      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â”‚
                         â”‚                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”
                         â”‚                  â”‚                 â”‚
                         â”‚           â‰¥ 70% similar    < 70% similar
                         â”‚                  â”‚                 â”‚
                         â”‚                  â–¼                 â–¼
                         â”‚       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                         â”‚       â”‚ Use Existing    â”‚ â”‚ Adapt Strategy  â”‚
                         â”‚       â”‚ Strategy        â”‚ â”‚ â€¢ Re-infer      â”‚
                         â”‚       â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â€¢ Save new      â”‚
                         â”‚                â”‚          â”‚ â€¢ Log change    â”‚
                         â”‚                â”‚          â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â–¼                â–¼                   â”‚
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚              EXTRACT CONTENT                      â”‚
              â”‚  Apply CSS selectors to get title, content, etc.  â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Sitemap Processing

The crawler includes comprehensive XML sitemap support for efficient URL discovery. Instead of crawling an entire site link-by-link, sitemaps provide a structured index of all pages a site wants indexed.

### What is a Sitemap?

XML sitemaps are files that list URLs for a site along with metadata about each URL (when it was last updated, how often it changes, how important it is relative to other URLs). Search engines use sitemaps to crawl sites more efficiently.

### Sitemap Types Supported

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    SITEMAP FORMATS                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                  â”‚
â”‚  1. URLSET (Standard Sitemap)                                   â”‚
â”‚     â””â”€â”€â–º Contains individual URLs with metadata                 â”‚
â”‚                                                                  â”‚
â”‚     <?xml version="1.0" encoding="UTF-8"?>                      â”‚
â”‚     <urlset xmlns="http://www.sitemaps.org/schemas/sitemap/0.9">â”‚
â”‚       <url>                                                      â”‚
â”‚         <loc>https://example.com/page1</loc>                    â”‚
â”‚         <lastmod>2025-01-15</lastmod>                           â”‚
â”‚         <changefreq>weekly</changefreq>                         â”‚
â”‚         <priority>0.8</priority>                                â”‚
â”‚       </url>                                                     â”‚
â”‚     </urlset>                                                    â”‚
â”‚                                                                  â”‚
â”‚  2. SITEMAPINDEX (Sitemap Index)                                â”‚
â”‚     â””â”€â”€â–º Points to multiple child sitemaps                      â”‚
â”‚     â””â”€â”€â–º Used by large sites (50,000+ URLs)                     â”‚
â”‚                                                                  â”‚
â”‚     <sitemapindex xmlns="...">                                  â”‚
â”‚       <sitemap>                                                  â”‚
â”‚         <loc>https://example.com/sitemap-articles.xml</loc>     â”‚
â”‚         <lastmod>2025-01-20</lastmod>                           â”‚
â”‚       </sitemap>                                                 â”‚
â”‚       <sitemap>                                                  â”‚
â”‚         <loc>https://example.com/sitemap-products.xml</loc>     â”‚
â”‚       </sitemap>                                                 â”‚
â”‚     </sitemapindex>                                              â”‚
â”‚                                                                  â”‚
â”‚  3. GZIP COMPRESSED (.xml.gz)                                   â”‚
â”‚     â””â”€â”€â–º Automatically detected and decompressed                â”‚
â”‚     â””â”€â”€â–º Common for large sitemaps                              â”‚
â”‚                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### URL Metadata Fields

| Field | Description | Example |
|-------|-------------|---------|
| `loc` | The URL (required) | `https://example.com/page` |
| `lastmod` | Last modification date | `2025-01-15` or `2025-01-15T10:30:00Z` |
| `changefreq` | How often the page changes | `always`, `hourly`, `daily`, `weekly`, `monthly`, `yearly`, `never` |
| `priority` | Relative importance (0.0-1.0) | `0.8` (higher = more important) |

### How Sitemap Processing Works

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    SITEMAP PROCESSING FLOW                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                  â”‚
â”‚  1. DISCOVERY                                                    â”‚
â”‚     â”œâ”€â”€â–º Check robots.txt for Sitemap: directives               â”‚
â”‚     â””â”€â”€â–º Try common paths: /sitemap.xml, /sitemap_index.xml     â”‚
â”‚                                                                  â”‚
â”‚  2. FETCH                                                        â”‚
â”‚     â”œâ”€â”€â–º HTTP GET with User-Agent                               â”‚
â”‚     â”œâ”€â”€â–º Handle gzip compression                                â”‚
â”‚     â””â”€â”€â–º Follow redirects                                        â”‚
â”‚                                                                  â”‚
â”‚  3. PARSE                                                        â”‚
â”‚     â”œâ”€â”€â–º Detect type (urlset vs sitemapindex)                   â”‚
â”‚     â”œâ”€â”€â–º Extract URLs and metadata                              â”‚
â”‚     â””â”€â”€â–º Validate against sitemap protocol                      â”‚
â”‚                                                                  â”‚
â”‚  4. RECURSE (for sitemap indexes)                               â”‚
â”‚     â”œâ”€â”€â–º Queue child sitemaps                                   â”‚
â”‚     â”œâ”€â”€â–º Track processed sitemaps (avoid duplicates)            â”‚
â”‚     â””â”€â”€â–º Respect max_sitemaps limit                             â”‚
â”‚                                                                  â”‚
â”‚  5. YIELD URLS                                                   â”‚
â”‚     â”œâ”€â”€â–º Stream URLs as discovered                              â”‚
â”‚     â”œâ”€â”€â–º Include metadata (lastmod, changefreq, priority)       â”‚
â”‚     â””â”€â”€â–º Filter by domain if specified                          â”‚
â”‚                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Python API

```python
from crawler.compliance import (
    SitemapFetcher,
    SitemapParser,
    fetch_sitemap_urls,
    ChangeFrequency,
)

# Quick fetch all URLs from a domain's sitemaps
urls = await fetch_sitemap_urls(
    domain="example.com",
    user_agent="MyCrawler/1.0",
    timeout=30.0,
)
for url in urls:
    print(f"{url.loc} - last modified: {url.lastmod}")

# Full control with SitemapFetcher
async with SitemapFetcher(
    user_agent="MyCrawler/1.0",
    max_sitemaps=100,           # Max sitemap files to process
    max_urls_per_sitemap=50000, # Max URLs per sitemap
) as fetcher:
    # Discover sitemaps for a domain
    sitemap_urls = await fetcher.discover_sitemaps("example.com")

    # Fetch and parse all sitemaps (handles indexes recursively)
    async for sitemap in fetcher.fetch_all_sitemaps(sitemap_urls):
        print(f"Sitemap: {sitemap.url}")
        print(f"  Is index: {sitemap.is_index}")
        print(f"  URLs: {len(sitemap.urls)}")
        print(f"  Child sitemaps: {len(sitemap.sitemaps)}")

        # Access individual URLs
        for url in sitemap.urls:
            print(f"  - {url.loc}")
            if url.changefreq == ChangeFrequency.DAILY:
                print("    (updates daily)")

# Parse sitemap content directly
parser = SitemapParser()
sitemap = parser.parse(
    content=xml_bytes,
    url="https://example.com/sitemap.xml",
    status_code=200,
)
```

### Integration with Crawler

```python
from crawler.core import Crawler
from crawler.compliance import SitemapFetcher

async def crawl_from_sitemap():
    # Fetch URLs from sitemap first
    async with SitemapFetcher() as fetcher:
        sitemap_urls = await fetcher.discover_sitemaps("example.com")
        seed_urls = []
        async for url in fetcher.get_all_urls(sitemap_urls):
            seed_urls.append(url.loc)

    # Use sitemap URLs as seeds
    config = CrawlConfig(
        seed_urls=seed_urls[:1000],  # Limit initial seeds
        output_dir="./data",
        max_pages=5000,
    )

    async with Crawler(config) as crawler:
        stats = await crawler.crawl()
```

---

## JavaScript Rendering

Modern websites often rely heavily on JavaScript to render content. Single Page Applications (SPAs) built with React, Vue, Angular, or similar frameworks may show only a loading spinner when fetched with a simple HTTP request. The JavaScript Rendering module uses Playwright to execute JavaScript and capture the fully-rendered DOM.

### When JS Rendering is Needed

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 JS RENDERING DETECTION                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                  â”‚
â”‚  The crawler automatically detects when JS rendering is needed  â”‚
â”‚  by looking for common SPA framework patterns:                  â”‚
â”‚                                                                  â”‚
â”‚  REACT                          VUE                             â”‚
â”‚  â”€â”€â”€â”€â”€                          â”€â”€â”€                             â”‚
â”‚  â€¢ <div id="root"></div>        â€¢ <div id="app"></div>         â”‚
â”‚  â€¢ data-reactroot               â€¢ data-v- attributes            â”‚
â”‚  â€¢ __NEXT_DATA__ (Next.js)      â€¢ __NUXT__ (Nuxt.js)           â”‚
â”‚                                                                  â”‚
â”‚  ANGULAR                        SVELTE                          â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€                        â”€â”€â”€â”€â”€â”€                          â”‚
â”‚  â€¢ ng-app attribute             â€¢ svelte- classes               â”‚
â”‚  â€¢ _nghost attributes           â€¢ __svelte_                     â”‚
â”‚  â€¢ ng-version                                                    â”‚
â”‚                                                                  â”‚
â”‚  GENERIC SPA INDICATORS                                         â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                          â”‚
â”‚  â€¢ Empty body with JS includes                                  â”‚
â”‚  â€¢ "Loading..." placeholder text                                â”‚
â”‚  â€¢ Minimal HTML with large JS bundles                           â”‚
â”‚                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 JS RENDERING ARCHITECTURE                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                  â”‚
â”‚                      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                        â”‚
â”‚                      â”‚  HybridFetcher  â”‚                        â”‚
â”‚                      â”‚  (Entry Point)  â”‚                        â”‚
â”‚                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                        â”‚
â”‚                               â”‚                                  â”‚
â”‚            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
â”‚            â”‚                  â”‚                  â”‚              â”‚
â”‚            â–¼                  â–¼                  â–¼              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚   HTTP Fetch    â”‚ â”‚ JS Requirement  â”‚ â”‚   JSRenderer    â”‚  â”‚
â”‚  â”‚   (Fast Path)   â”‚ â”‚    Detector     â”‚ â”‚  (Slow Path)    â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚           â”‚                   â”‚                   â”‚            â”‚
â”‚           â”‚          â”Œâ”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”           â”‚            â”‚
â”‚           â”‚          â”‚ Needs JS?     â”‚           â”‚            â”‚
â”‚           â”‚          â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚            â”‚
â”‚           â”‚                  â”‚                   â”‚            â”‚
â”‚           â”‚        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚            â”‚
â”‚           â”‚        â”‚                 â”‚          â”‚            â”‚
â”‚           â”‚       NO                YES         â”‚            â”‚
â”‚           â”‚        â”‚                 â”‚          â”‚            â”‚
â”‚           â–¼        â–¼                 â–¼          â”‚            â”‚
â”‚      Return HTML directly      Use JSRenderer â—„â”€â”˜            â”‚
â”‚                                      â”‚                        â”‚
â”‚                                      â–¼                        â”‚
â”‚                              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
â”‚                              â”‚  BrowserPool    â”‚              â”‚
â”‚                              â”‚                 â”‚              â”‚
â”‚                              â”‚ â€¢ Chromium      â”‚              â”‚
â”‚                              â”‚ â€¢ Firefox       â”‚              â”‚
â”‚                              â”‚ â€¢ WebKit        â”‚              â”‚
â”‚                              â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
â”‚                                       â”‚                        â”‚
â”‚                                       â–¼                        â”‚
â”‚                              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
â”‚                              â”‚ Rendered HTML   â”‚              â”‚
â”‚                              â”‚ + Screenshots   â”‚              â”‚
â”‚                              â”‚ + Console Logs  â”‚              â”‚
â”‚                              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
â”‚                                                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Wait Strategies

The renderer supports multiple strategies for determining when a page is "ready":

| Strategy | Description | Best For |
|----------|-------------|----------|
| `load` | Wait for window.onload | Simple pages |
| `domcontentloaded` | Wait for DOMContentLoaded | Static content |
| `networkidle` | Wait until no network requests for 500ms | API-heavy SPAs |
| `selector` | Wait for specific CSS selector to appear | Known content markers |
| `function` | Wait for custom JS function to return true | Complex conditions |

### Python API

```python
from crawler.core import (
    JSRenderer,
    BrowserPool,
    HybridFetcher,
    JSRequirementDetector,
    RenderConfig,
    WaitStrategy,
)

# Basic rendering
async with JSRenderer() as renderer:
    result = await renderer.render("https://spa-example.com")
    print(f"Status: {result.status_code}")
    print(f"HTML length: {len(result.html)}")
    print(f"Render time: {result.render_time_ms}ms")
    print(f"Console logs: {result.console_logs}")

# With custom configuration
config = RenderConfig(
    wait_strategy=WaitStrategy.NETWORKIDLE,
    timeout_ms=30000,
    viewport_width=1920,
    viewport_height=1080,
    user_agent="MyBot/1.0",
    block_resources=["image", "media", "font"],  # Speed up rendering
    capture_screenshot=True,
)

async with JSRenderer() as renderer:
    result = await renderer.render("https://example.com", config)
    if result.screenshot:
        with open("screenshot.png", "wb") as f:
            f.write(result.screenshot)

# Smart rendering (only when needed)
async with JSRenderer() as renderer:
    # First fetch with HTTP
    http_html = "<html>..."  # From regular HTTP fetch

    # Check if JS rendering is needed
    result = await renderer.render_if_needed(
        url="https://example.com",
        initial_html=http_html,
    )

    if result.was_rendered:
        print("Page required JS rendering")
    else:
        print("HTTP fetch was sufficient")

# Browser pool for high-volume rendering
async with BrowserPool(
    max_browsers=3,
    max_contexts_per_browser=5,
    browser_type="chromium",  # or "firefox", "webkit"
) as pool:
    # Acquire browser context
    async with pool.acquire() as context:
        page = await context.new_page()
        await page.goto("https://example.com")
        content = await page.content()

# Hybrid fetcher (combines HTTP + JS rendering)
async with HybridFetcher(
    js_renderer=renderer,
    http_timeout=10.0,
) as fetcher:
    # Automatically uses JS rendering when needed
    result = await fetcher.fetch("https://spa-example.com")
    print(f"Used JS: {result.used_js_rendering}")
    print(f"HTML: {result.html[:500]}...")
```

### Installation

```bash
# Install Playwright
pip install playwright

# Install browser binaries
playwright install chromium  # or: playwright install firefox webkit
```

### Configuration Options

| Option | Default | Description |
|--------|---------|-------------|
| `browser_type` | `chromium` | Browser engine: `chromium`, `firefox`, `webkit` |
| `headless` | `True` | Run browser without GUI |
| `timeout_ms` | `30000` | Page load timeout |
| `wait_strategy` | `networkidle` | When to consider page loaded |
| `viewport_width` | `1280` | Browser viewport width |
| `viewport_height` | `720` | Browser viewport height |
| `block_resources` | `[]` | Resource types to block for speed |
| `capture_screenshot` | `False` | Take screenshot after render |
| `capture_console` | `True` | Capture browser console logs |

---

## Distributed Crawling

For large-scale crawling operations, the distributed crawling system enables multiple workers to coordinate and process URLs in parallel across machines.

### Architecture Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 DISTRIBUTED CRAWLING SYSTEM                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                  â”‚
â”‚                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                      â”‚
â”‚                    â”‚ DistributedCrawl    â”‚                      â”‚
â”‚                    â”‚     Manager         â”‚                      â”‚
â”‚                    â”‚                     â”‚                      â”‚
â”‚                    â”‚ â€¢ Create jobs       â”‚                      â”‚
â”‚                    â”‚ â€¢ Monitor progress  â”‚                      â”‚
â”‚                    â”‚ â€¢ Collect results   â”‚                      â”‚
â”‚                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                      â”‚
â”‚                               â”‚                                  â”‚
â”‚                               â–¼                                  â”‚
â”‚                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                      â”‚
â”‚                    â”‚       REDIS         â”‚                      â”‚
â”‚                    â”‚                     â”‚                      â”‚
â”‚                    â”‚ â€¢ URL Queue         â”‚                      â”‚
â”‚                    â”‚ â€¢ Worker Registry   â”‚                      â”‚
â”‚                    â”‚ â€¢ Job State         â”‚                      â”‚
â”‚                    â”‚ â€¢ Leader Lock       â”‚                      â”‚
â”‚                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                      â”‚
â”‚                               â”‚                                  â”‚
â”‚          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚
â”‚          â”‚                    â”‚                    â”‚            â”‚
â”‚          â–¼                    â–¼                    â–¼            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚   Worker 1    â”‚   â”‚   Worker 2    â”‚   â”‚   Worker 3    â”‚    â”‚
â”‚  â”‚   (Leader)    â”‚   â”‚               â”‚   â”‚               â”‚    â”‚
â”‚  â”‚               â”‚   â”‚               â”‚   â”‚               â”‚    â”‚
â”‚  â”‚ â€¢ Claim URLs  â”‚   â”‚ â€¢ Claim URLs  â”‚   â”‚ â€¢ Claim URLs  â”‚    â”‚
â”‚  â”‚ â€¢ Fetch pages â”‚   â”‚ â€¢ Fetch pages â”‚   â”‚ â€¢ Fetch pages â”‚    â”‚
â”‚  â”‚ â€¢ Heartbeat   â”‚   â”‚ â€¢ Heartbeat   â”‚   â”‚ â€¢ Heartbeat   â”‚    â”‚
â”‚  â”‚ â€¢ Coordinate  â”‚   â”‚               â”‚   â”‚               â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Key Components

#### 1. DistributedQueue

The URL queue manages URL distribution across workers with atomic operations:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    DISTRIBUTED QUEUE                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                  â”‚
â”‚  OPERATIONS (all atomic)                                        â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                        â”‚
â”‚                                                                  â”‚
â”‚  add_url(task)                                                   â”‚
â”‚    â””â”€â”€â–º Add URL if not already queued/visited                   â”‚
â”‚    â””â”€â”€â–º Sets priority for ordering                              â”‚
â”‚                                                                  â”‚
â”‚  claim_url(worker_id)                                           â”‚
â”‚    â””â”€â”€â–º Atomically pop highest priority URL                     â”‚
â”‚    â””â”€â”€â–º Mark as processing by this worker                       â”‚
â”‚    â””â”€â”€â–º Set claim timestamp for timeout detection               â”‚
â”‚                                                                  â”‚
â”‚  complete_url(url, success)                                     â”‚
â”‚    â””â”€â”€â–º Mark URL as completed/failed                            â”‚
â”‚    â””â”€â”€â–º Release from processing state                           â”‚
â”‚                                                                  â”‚
â”‚  recover_stale_urls()                                           â”‚
â”‚    â””â”€â”€â–º Find URLs claimed but not completed (timeout)           â”‚
â”‚    â””â”€â”€â–º Re-queue for another worker to process                  â”‚
â”‚                                                                  â”‚
â”‚  REDIS KEYS                                                      â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                      â”‚
â”‚  job:{id}:pending     - Sorted set (priority queue)             â”‚
â”‚  job:{id}:processing  - Hash (url -> worker_id)                 â”‚
â”‚  job:{id}:completed   - Set (finished URLs)                     â”‚
â”‚  job:{id}:failed      - Set (failed URLs)                       â”‚
â”‚  job:{id}:seen        - Set (all URLs ever added)               â”‚
â”‚                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### 2. WorkerCoordinator

Manages worker registration, heartbeats, and leader election:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    WORKER COORDINATION                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                  â”‚
â”‚  HEARTBEATS                                                      â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                      â”‚
â”‚  â€¢ Workers send heartbeat every N seconds                       â”‚
â”‚  â€¢ Heartbeat includes: URLs processed, errors, last activity    â”‚
â”‚  â€¢ Missing heartbeats = worker presumed dead                    â”‚
â”‚                                                                  â”‚
â”‚  LEADER ELECTION                                                 â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                 â”‚
â”‚  â€¢ Redis SETNX for distributed lock                             â”‚
â”‚  â€¢ Leader performs coordination tasks:                          â”‚
â”‚    - Cleanup dead workers                                       â”‚
â”‚    - Recover stale URLs                                         â”‚
â”‚    - Publish global stats                                       â”‚
â”‚  â€¢ Lock auto-expires (TTL) if leader dies                       â”‚
â”‚                                                                  â”‚
â”‚  WORKER STATES                                                   â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                   â”‚
â”‚  IDLE       â†’ Waiting for work                                  â”‚
â”‚  ACTIVE     â†’ Processing URLs                                   â”‚
â”‚  PAUSED     â†’ Temporarily stopped                               â”‚
â”‚  STOPPING   â†’ Graceful shutdown in progress                     â”‚
â”‚  DEAD       â†’ No heartbeat, presumed failed                     â”‚
â”‚                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Python API

```python
from crawler.core import (
    DistributedQueue,
    DistributedCrawlManager,
    CrawlerWorker,
    WorkerCoordinator,
    CrawlJob,
    URLTask,
    JobState,
)
import redis.asyncio as redis

# Create a distributed crawl job
async def create_job():
    redis_client = redis.from_url("redis://localhost:6379/0")

    manager = DistributedCrawlManager(redis_client)

    # Create job with seed URLs
    job = await manager.create_job(
        seed_urls=["https://example.com", "https://example.com/about"],
        job_id="my-crawl-001",
        max_urls=10000,
        max_depth=5,
    )

    print(f"Job created: {job.job_id}")
    print(f"State: {job.state}")

    return job

# Run a worker
async def run_worker(job_id: str):
    redis_client = redis.from_url("redis://localhost:6379/0")

    worker = CrawlerWorker(
        redis_client=redis_client,
        job_id=job_id,
        worker_id="worker-1",  # Unique per worker
        heartbeat_interval=5.0,
        claim_timeout=300.0,   # 5 minutes to process a URL
    )

    # Start worker (runs until job complete or stopped)
    await worker.start()

# Monitor job progress
async def monitor_job(job_id: str):
    redis_client = redis.from_url("redis://localhost:6379/0")
    manager = DistributedCrawlManager(redis_client)

    while True:
        status = await manager.get_job_status(job_id)

        print(f"Pending: {status['pending_urls']}")
        print(f"Processing: {status['processing_urls']}")
        print(f"Completed: {status['completed_urls']}")
        print(f"Failed: {status['failed_urls']}")
        print(f"Workers: {status['active_workers']}")

        if status['state'] == JobState.COMPLETED:
            break

        await asyncio.sleep(5)

# Direct queue operations
async def queue_operations():
    redis_client = redis.from_url("redis://localhost:6379/0")
    queue = DistributedQueue(redis_client, job_id="my-crawl-001")

    # Add URLs with priority
    await queue.add_url(URLTask(
        url="https://example.com/important",
        depth=0,
        priority=10,  # Higher = processed first
    ))

    # Claim a URL for processing
    task = await queue.claim_url(worker_id="worker-1")
    if task:
        print(f"Claimed: {task.url}")

        # Process the URL...

        # Mark as complete
        await queue.complete_url(task.url, success=True)

    # Recover timed-out URLs
    recovered = await queue.recover_stale_urls()
    print(f"Recovered {recovered} stale URLs")
```

### Running Multiple Workers

```bash
# Terminal 1: Create job
python -c "
import asyncio
from my_crawler import create_job
asyncio.run(create_job())
"

# Terminal 2: Worker 1
WORKER_ID=worker-1 python -m crawler.worker --job-id my-crawl-001

# Terminal 3: Worker 2
WORKER_ID=worker-2 python -m crawler.worker --job-id my-crawl-001

# Terminal 4: Worker 3 (on different machine)
WORKER_ID=worker-3 REDIS_URL=redis://192.168.1.100:6379 \
    python -m crawler.worker --job-id my-crawl-001
```

### Job States

| State | Description |
|-------|-------------|
| `PENDING` | Job created, not started |
| `RUNNING` | Workers actively processing |
| `PAUSED` | Temporarily stopped |
| `COMPLETED` | All URLs processed |
| `FAILED` | Job failed (too many errors) |
| `CANCELLED` | Manually cancelled |

---

## Scheduled Recrawling

The scheduled recrawling system enables periodic re-crawling of URLs to detect content changes. It supports cron-like scheduling, adaptive intervals based on change frequency, and sitemap-based scheduling hints.

### Why Scheduled Recrawling?

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    USE CASES                                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                  â”‚
â”‚  1. NEWS MONITORING                                              â”‚
â”‚     â””â”€â”€â–º Check news sites every 15 minutes for new articles     â”‚
â”‚                                                                  â”‚
â”‚  2. PRICE TRACKING                                               â”‚
â”‚     â””â”€â”€â–º Monitor e-commerce prices daily                        â”‚
â”‚                                                                  â”‚
â”‚  3. COMPLIANCE CHECKING                                          â”‚
â”‚     â””â”€â”€â–º Verify terms of service weekly                         â”‚
â”‚                                                                  â”‚
â”‚  4. SEO MONITORING                                               â”‚
â”‚     â””â”€â”€â–º Track competitor content changes                       â”‚
â”‚                                                                  â”‚
â”‚  5. ARCHIVAL                                                     â”‚
â”‚     â””â”€â”€â–º Capture snapshots of pages over time                   â”‚
â”‚                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Scheduling Options

#### 1. Cron Expressions

Standard cron syntax for precise scheduling:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ minute (0 - 59)
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ hour (0 - 23)
â”‚ â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ day of month (1 - 31)
â”‚ â”‚ â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ month (1 - 12)
â”‚ â”‚ â”‚ â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ day of week (0 - 6, 0 = Sunday)
â”‚ â”‚ â”‚ â”‚ â”‚
â”‚ â”‚ â”‚ â”‚ â”‚
* * * * *

Examples:
â”€â”€â”€â”€â”€â”€â”€â”€â”€
0 * * * *       Every hour at minute 0
*/15 * * * *    Every 15 minutes
0 9 * * 1-5     9 AM on weekdays
0 0 1 * *       First day of each month at midnight
0 */6 * * *     Every 6 hours
```

#### 2. Interval-Based

Simple time intervals:

```python
from crawler.core import ScheduleInterval

# Predefined intervals
ScheduleInterval.MINUTES_15    # Every 15 minutes
ScheduleInterval.HOURLY        # Every hour
ScheduleInterval.DAILY         # Once a day
ScheduleInterval.WEEKLY        # Once a week
ScheduleInterval.MONTHLY       # Once a month
```

#### 3. Adaptive Scheduling

Automatically adjusts recrawl frequency based on how often content actually changes:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    ADAPTIVE SCHEDULING                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                  â”‚
â”‚  Initial interval: 1 hour                                       â”‚
â”‚                                                                  â”‚
â”‚  Crawl 1: No change   â†’ Interval Ã— 1.5 = 1.5 hours             â”‚
â”‚  Crawl 2: No change   â†’ Interval Ã— 1.5 = 2.25 hours            â”‚
â”‚  Crawl 3: CHANGED!    â†’ Interval Ã— 0.5 = 1.125 hours           â”‚
â”‚  Crawl 4: No change   â†’ Interval Ã— 1.5 = 1.69 hours            â”‚
â”‚  ...                                                             â”‚
â”‚                                                                  â”‚
â”‚  Bounds:                                                         â”‚
â”‚  â€¢ Min interval: 15 minutes (never faster)                      â”‚
â”‚  â€¢ Max interval: 7 days (never slower)                          â”‚
â”‚                                                                  â”‚
â”‚  Benefits:                                                       â”‚
â”‚  â€¢ Frequently-changing pages crawled more often                 â”‚
â”‚  â€¢ Stable pages crawled less often                              â”‚
â”‚  â€¢ Automatically optimizes crawl resources                      â”‚
â”‚                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 RECRAWL SCHEDULER SYSTEM                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                  â”‚
â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                      â”‚
â”‚              â”‚     RecrawlScheduler      â”‚                      â”‚
â”‚              â”‚                           â”‚                      â”‚
â”‚              â”‚ â€¢ Manage URL schedules    â”‚                      â”‚
â”‚              â”‚ â€¢ Check for due URLs      â”‚                      â”‚
â”‚              â”‚ â€¢ Trigger recrawls        â”‚                      â”‚
â”‚              â”‚ â€¢ Update intervals        â”‚                      â”‚
â”‚              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                      â”‚
â”‚                            â”‚                                     â”‚
â”‚            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”‚
â”‚            â”‚               â”‚               â”‚                    â”‚
â”‚            â–¼               â–¼               â–¼                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚  â”‚  CronSchedule   â”‚ â”‚  Redis    â”‚ â”‚ SitemapBased    â”‚        â”‚
â”‚  â”‚                 â”‚ â”‚  Store    â”‚ â”‚   Scheduler     â”‚        â”‚
â”‚  â”‚ â€¢ Parse cron    â”‚ â”‚           â”‚ â”‚                 â”‚        â”‚
â”‚  â”‚ â€¢ Next run time â”‚ â”‚ Schedules â”‚ â”‚ â€¢ Use lastmod   â”‚        â”‚
â”‚  â”‚ â€¢ Validate      â”‚ â”‚ History   â”‚ â”‚ â€¢ Use changefreqâ”‚        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ Metrics   â”‚ â”‚ â€¢ Batch scheduleâ”‚        â”‚
â”‚                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â”‚                                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚                  SCHEDULE RECORD                         â”‚   â”‚
â”‚  â”‚                                                          â”‚   â”‚
â”‚  â”‚  url: https://example.com/page                          â”‚   â”‚
â”‚  â”‚  schedule_type: cron | interval | adaptive              â”‚   â”‚
â”‚  â”‚  cron_expr: "0 */6 * * *"                               â”‚   â”‚
â”‚  â”‚  interval_seconds: 21600                                â”‚   â”‚
â”‚  â”‚  last_crawled: 2025-01-30T10:00:00Z                     â”‚   â”‚
â”‚  â”‚  next_crawl: 2025-01-30T16:00:00Z                       â”‚   â”‚
â”‚  â”‚  consecutive_no_change: 3                               â”‚   â”‚
â”‚  â”‚  total_crawls: 47                                       â”‚   â”‚
â”‚  â”‚  total_changes: 12                                      â”‚   â”‚
â”‚  â”‚  enabled: true                                          â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Python API

```python
from crawler.core import (
    RecrawlScheduler,
    RecrawlSchedule,
    CronSchedule,
    ScheduleInterval,
    AdaptiveScheduleConfig,
    SitemapBasedScheduler,
)
import redis.asyncio as redis

# Basic cron scheduling
async def schedule_with_cron():
    redis_client = redis.from_url("redis://localhost:6379/0")
    scheduler = RecrawlScheduler(redis_client)

    # Schedule URL with cron expression
    schedule = await scheduler.add_url_schedule(
        url="https://news.example.com",
        interval="0 */2 * * *",  # Every 2 hours
    )
    print(f"Next crawl: {schedule.next_crawl}")

# Interval-based scheduling
async def schedule_with_interval():
    redis_client = redis.from_url("redis://localhost:6379/0")
    scheduler = RecrawlScheduler(redis_client)

    # Schedule with predefined interval
    await scheduler.add_url_schedule(
        url="https://blog.example.com",
        interval=ScheduleInterval.DAILY,
    )

    # Or with custom seconds
    await scheduler.add_url_schedule(
        url="https://prices.example.com",
        interval=3600,  # Every hour
    )

# Adaptive scheduling
async def schedule_adaptive():
    redis_client = redis.from_url("redis://localhost:6379/0")

    adaptive_config = AdaptiveScheduleConfig(
        initial_interval=3600,      # Start with 1 hour
        min_interval=900,           # Never faster than 15 minutes
        max_interval=604800,        # Never slower than 1 week
        increase_factor=1.5,        # Slow down by 50% when unchanged
        decrease_factor=0.5,        # Speed up by 50% when changed
    )

    scheduler = RecrawlScheduler(
        redis_client,
        adaptive_config=adaptive_config,
    )

    await scheduler.add_url_schedule(
        url="https://example.com",
        interval=ScheduleInterval.HOURLY,
        adaptive=True,  # Enable adaptive adjustment
    )

# Sitemap-based scheduling
async def schedule_from_sitemap():
    redis_client = redis.from_url("redis://localhost:6379/0")

    sitemap_scheduler = SitemapBasedScheduler(redis_client)

    # Import schedules from sitemap
    await sitemap_scheduler.import_from_sitemap(
        sitemap_url="https://example.com/sitemap.xml",
        default_interval=ScheduleInterval.DAILY,
    )
    # Uses sitemap's changefreq and lastmod to set intelligent intervals:
    # - changefreq: "hourly" â†’ 1 hour interval
    # - changefreq: "daily" â†’ 24 hour interval
    # - lastmod: recent â†’ shorter interval

# Run the scheduler
async def run_scheduler():
    redis_client = redis.from_url("redis://localhost:6379/0")
    scheduler = RecrawlScheduler(redis_client)

    # Define what happens when a URL is due
    async def on_url_due(schedule: RecrawlSchedule):
        print(f"Time to recrawl: {schedule.url}")

        # Perform the crawl...
        content_changed = await crawl_and_check(schedule.url)

        # Report result (updates adaptive interval)
        await scheduler.record_crawl_result(
            url=schedule.url,
            changed=content_changed,
        )

    # Start scheduler loop
    await scheduler.run(
        callback=on_url_due,
        check_interval=60,  # Check for due URLs every minute
    )

# Query schedules
async def query_schedules():
    redis_client = redis.from_url("redis://localhost:6379/0")
    scheduler = RecrawlScheduler(redis_client)

    # Get all schedules
    all_schedules = await scheduler.list_schedules()

    # Get schedules due now
    due_now = await scheduler.get_due_urls()

    # Get schedule for specific URL
    schedule = await scheduler.get_schedule("https://example.com")
    print(f"URL: {schedule.url}")
    print(f"Last crawled: {schedule.last_crawled}")
    print(f"Next crawl: {schedule.next_crawl}")
    print(f"Change rate: {schedule.total_changes}/{schedule.total_crawls}")

    # Disable/enable schedule
    await scheduler.disable_schedule("https://example.com")
    await scheduler.enable_schedule("https://example.com")

    # Remove schedule
    await scheduler.remove_schedule("https://example.com")
```

### Cron Expression Reference

| Expression | Description |
|------------|-------------|
| `* * * * *` | Every minute |
| `*/5 * * * *` | Every 5 minutes |
| `0 * * * *` | Every hour |
| `0 */2 * * *` | Every 2 hours |
| `0 0 * * *` | Daily at midnight |
| `0 9 * * 1-5` | Weekdays at 9 AM |
| `0 0 * * 0` | Weekly on Sunday |
| `0 0 1 * *` | Monthly on the 1st |
| `0 0 1 1 *` | Yearly on Jan 1st |

### Change Frequency to Interval Mapping

When using sitemap-based scheduling, `changefreq` values are mapped to intervals:

| changefreq | Default Interval |
|------------|------------------|
| `always` | 1 hour |
| `hourly` | 1 hour |
| `daily` | 24 hours |
| `weekly` | 7 days |
| `monthly` | 30 days |
| `yearly` | 365 days |
| `never` | Not scheduled |

---

## Configuration

### Environment Variables

```bash
# .env file

# Required
REDIS_URL=redis://localhost:6379/0

# Crawler identity
CRAWLER_USER_AGENT=MyCrawler/1.0 (+https://mysite.com/bot; bot@mysite.com)

# Rate limiting
CRAWLER_DEFAULT_DELAY=1.0      # Seconds between requests per domain
CRAWLER_MAX_CONCURRENT=10      # Global concurrent connections

# GDPR/Privacy
GDPR_ENABLED=true
GDPR_RETENTION_DAYS=365
PII_HANDLING=redact            # redact, pseudonymize, or exclude_page

# Adaptive features
ENABLE_EMBEDDINGS=false
EMBEDDING_MODEL=all-MiniLM-L6-v2
```

### Python Configuration

```python
from crawler.config import (
    CrawlConfig,
    RateLimitConfig,
    SafetyLimits,
    GDPRConfig,
    PIIHandlingConfig,
)

config = CrawlConfig(
    # Required
    seed_urls=["https://example.com"],
    output_dir="./data",

    # Crawl limits
    max_depth=10,                    # How deep to crawl
    max_pages=1000,                  # Total page limit
    max_pages_per_domain=100,        # Per-domain limit
    allowed_domains=["example.com"], # Restrict to domains
    exclude_patterns=["/admin/"],    # Skip these paths

    # Rate limiting
    rate_limit=RateLimitConfig(
        default_delay=1.0,           # Base delay (seconds)
        min_delay=0.5,               # Minimum delay
        max_delay=60.0,              # Maximum backoff
        adaptive=True,               # Auto-adjust on 429/503
        respect_crawl_delay=True,    # Honor robots.txt
    ),

    # Safety
    safety=SafetyLimits(
        max_page_size_mb=10.0,       # Skip large pages
        request_timeout_seconds=30,  # Per-request timeout
        max_retries=3,               # Retry failed requests
    ),

    # GDPR compliance
    gdpr=GDPRConfig(
        enabled=True,
        retention_days=365,
        collect_only=["url", "title", "content"],
    ),

    # PII handling
    pii=PIIHandlingConfig(
        action="redact",             # What to do with PII
        log_detections=True,         # Audit trail
    ),
)
```

### Configuration Reference

| Category | Option | Default | Description |
|----------|--------|---------|-------------|
| **Crawl** | `max_depth` | 10 | Maximum link depth from seed |
| | `max_pages` | None | Total pages to crawl |
| | `max_pages_per_domain` | None | Per-domain page limit |
| | `allowed_domains` | [] | Restrict to these domains |
| | `exclude_patterns` | [] | URL patterns to skip |
| **Rate Limit** | `default_delay` | 1.0 | Seconds between requests |
| | `min_delay` | 0.5 | Minimum delay floor |
| | `max_delay` | 60.0 | Maximum backoff ceiling |
| | `adaptive` | True | Auto-adjust on rate limits |
| | `respect_crawl_delay` | True | Honor robots.txt delay |
| **Safety** | `max_page_size_mb` | 10.0 | Skip pages larger than |
| | `request_timeout_seconds` | 30 | Request timeout |
| | `max_retries` | 3 | Retry attempts |
| | `verify_ssl` | True | Verify SSL certificates |
| | `block_private_ips` | True | Block 192.168.x, etc. |
| **GDPR** | `enabled` | False | Enable GDPR compliance |
| | `retention_days` | 365 | Data retention period |
| | `exclude_pii_patterns` | True | Strip PII from content |
| **Storage** | `ttl_seconds` | 604800 | Structure cache TTL (7 days) |
| | `max_versions` | 10 | Keep structure history |

---

## Usage Examples

This section provides comprehensive examples for common crawling scenarios, from simple single-page crawls to complex multi-site monitoring.

### Command Line Usage

#### Basic Crawl

```bash
# Crawl a single site
python -m crawler \
    --seed-url https://example.com \
    --output ./data

# Multiple seed URLs
python -m crawler \
    --seed-url https://site1.com \
    --seed-url https://site2.com \
    --output ./data

# Read seed URLs from a file
cat seeds.txt
# https://example1.com
# https://example2.com
# https://example3.com

python -m crawler \
    --seed-file seeds.txt \
    --output ./data
```

#### Crawl Depth and Limits

```bash
# Limit crawl depth (how many links to follow from seed)
python -m crawler \
    --seed-url https://example.com \
    --output ./data \
    --max-depth 3

# Limit total pages
python -m crawler \
    --seed-url https://example.com \
    --output ./data \
    --max-pages 100

# Limit pages per domain (useful for multi-domain crawls)
python -m crawler \
    --seed-url https://example.com \
    --output ./data \
    --max-pages-per-domain 50

# Combined limits
python -m crawler \
    --seed-url https://example.com \
    --output ./data \
    --max-depth 5 \
    --max-pages 1000 \
    --max-pages-per-domain 200
```

#### Domain Restrictions

```bash
# Stay within specific domains
python -m crawler \
    --seed-url https://docs.example.com \
    --output ./data \
    --allowed-domains docs.example.com api.example.com

# Exclude specific paths
python -m crawler \
    --seed-url https://example.com \
    --output ./data \
    --exclude-patterns "/admin/*" "/private/*" "/api/*"

# Combine domain and path restrictions
python -m crawler \
    --seed-url https://example.com \
    --output ./data \
    --allowed-domains example.com \
    --exclude-patterns "/cdn/*" "*.pdf" "*.zip"
```

#### Rate Limiting and Politeness

```bash
# Set delay between requests (seconds)
python -m crawler \
    --seed-url https://example.com \
    --output ./data \
    --rate-limit 2.0

# Respect robots.txt (enabled by default)
python -m crawler \
    --seed-url https://example.com \
    --output ./data \
    --respect-robots

# Set custom User-Agent
python -m crawler \
    --seed-url https://example.com \
    --output ./data \
    --user-agent "MyCompanyBot/1.0 (+https://company.com/bot; contact@company.com)"

# Very polite crawl for sensitive sites
python -m crawler \
    --seed-url https://example.com \
    --output ./data \
    --rate-limit 5.0 \
    --max-concurrent 2 \
    --respect-robots \
    --user-agent "ResearchBot/1.0 (Academic research; contact@university.edu)"
```

### Python API Examples

#### Example 1: Simple Crawl

```python
import asyncio
from crawler.core.crawler import Crawler
from crawler.config import CrawlConfig

async def simple_crawl():
    """Basic crawl with minimal configuration."""
    config = CrawlConfig(
        seed_urls=["https://example.com"],
        output_dir="./data",
    )

    async with Crawler(config) as crawler:
        stats = await crawler.crawl()

    print(f"Crawled {stats.pages_crawled} pages")
    print(f"Found {stats.links_discovered} links")
    print(f"Errors: {stats.errors}")

asyncio.run(simple_crawl())
```

#### Example 2: Crawl with Callbacks

```python
import asyncio
from crawler.core.crawler import Crawler
from crawler.config import CrawlConfig

async def crawl_with_callbacks():
    """Crawl with real-time progress callbacks."""
    config = CrawlConfig(
        seed_urls=["https://example.com"],
        output_dir="./data",
        max_pages=50,
    )

    # Track progress
    pages_crawled = []
    errors = []

    async with Crawler(config) as crawler:
        # Register callbacks
        @crawler.on_page_crawled
        def handle_page(url: str, result):
            pages_crawled.append(url)
            print(f"âœ“ [{len(pages_crawled)}] {url}")
            print(f"  Title: {result.extracted.get('title', 'N/A')}")
            print(f"  Size: {result.content_length} bytes")

        @crawler.on_structure_learned
        def handle_structure(domain: str, page_type: str, structure):
            print(f"ğŸ“Š Learned structure for {domain}/{page_type}")
            print(f"   Tags: {len(structure.tag_hierarchy)} unique")
            print(f"   Classes: {len(structure.css_class_map)}")

        @crawler.on_error
        def handle_error(url: str, error: Exception):
            errors.append((url, error))
            print(f"âœ— Error: {url} - {error}")

        @crawler.on_rate_limited
        def handle_rate_limit(domain: str, delay: float):
            print(f"â³ Rate limited on {domain}, waiting {delay}s")

        stats = await crawler.crawl()

    print(f"\n=== Crawl Complete ===")
    print(f"Pages: {len(pages_crawled)}")
    print(f"Errors: {len(errors)}")

asyncio.run(crawl_with_callbacks())
```

#### Example 3: E-commerce Product Scraping

```python
import asyncio
import json
from crawler.core.crawler import Crawler
from crawler.config import CrawlConfig, RateLimitConfig
from crawler.extraction import ContentExtractor

async def scrape_products():
    """Scrape product pages from an e-commerce site."""
    config = CrawlConfig(
        seed_urls=["https://shop.example.com/products"],
        output_dir="./products",
        max_depth=3,
        max_pages=500,
        allowed_domains=["shop.example.com"],
        # Only crawl product pages
        include_patterns=["/products/*", "/product/*"],
        exclude_patterns=["/cart", "/checkout", "/account/*"],

        rate_limit=RateLimitConfig(
            default_delay=1.5,  # Be polite to the server
            adaptive=True,
        ),
    )

    products = []

    async with Crawler(config) as crawler:
        @crawler.on_page_crawled
        def extract_product(url: str, result):
            if "/product/" in url:
                # Custom product extraction
                product = {
                    "url": url,
                    "title": result.extracted.get("title"),
                    "price": extract_price(result.html),
                    "description": result.extracted.get("content"),
                    "images": result.extracted.get("images", []),
                    "sku": extract_sku(result.html),
                }
                products.append(product)
                print(f"Found product: {product['title']} - ${product['price']}")

        await crawler.crawl()

    # Save products
    with open("./products/products.json", "w") as f:
        json.dump(products, f, indent=2)

    print(f"Scraped {len(products)} products")

def extract_price(html: str) -> float:
    """Extract price from HTML (simplified example)."""
    import re
    match = re.search(r'\$(\d+(?:\.\d{2})?)', html)
    return float(match.group(1)) if match else 0.0

def extract_sku(html: str) -> str:
    """Extract SKU from HTML (simplified example)."""
    import re
    match = re.search(r'SKU:\s*(\w+)', html)
    return match.group(1) if match else ""

asyncio.run(scrape_products())
```

#### Example 4: News Article Monitoring

```python
import asyncio
from datetime import datetime
from crawler.core.crawler import Crawler
from crawler.config import CrawlConfig
from crawler.adaptive import ChangeDetector

async def monitor_news_site():
    """Monitor a news site for new articles."""
    config = CrawlConfig(
        seed_urls=["https://news.example.com"],
        output_dir="./news",
        max_depth=2,
        include_patterns=["/article/*", "/news/*", "/story/*"],
        exclude_patterns=["/archive/*", "/author/*"],
    )

    detector = ChangeDetector()
    new_articles = []

    async with Crawler(config) as crawler:
        @crawler.on_page_crawled
        def check_article(url: str, result):
            # Check if this is a new or updated article
            stored = crawler.structure_store.get(url)

            if stored is None:
                # New article
                new_articles.append({
                    "url": url,
                    "title": result.extracted.get("title"),
                    "published": datetime.now().isoformat(),
                    "status": "new",
                })
                print(f"ğŸ“° NEW: {result.extracted.get('title')}")
            else:
                # Check for updates
                analysis = detector.detect_changes(
                    stored.structure,
                    result.structure
                )
                if analysis.has_changes:
                    new_articles.append({
                        "url": url,
                        "title": result.extracted.get("title"),
                        "updated": datetime.now().isoformat(),
                        "status": "updated",
                        "change_type": analysis.classification.name,
                    })
                    print(f"ğŸ“ UPDATED: {result.extracted.get('title')}")

        await crawler.crawl()

    print(f"\nFound {len(new_articles)} new/updated articles")
    return new_articles

asyncio.run(monitor_news_site())
```

#### Example 5: Multi-Site Comparison

```python
import asyncio
from crawler.core.crawler import Crawler
from crawler.config import CrawlConfig

async def compare_sites():
    """Compare structure across multiple competitor sites."""
    sites = [
        "https://competitor1.com",
        "https://competitor2.com",
        "https://competitor3.com",
    ]

    results = {}

    for site in sites:
        config = CrawlConfig(
            seed_urls=[site],
            output_dir=f"./comparison/{site.split('//')[1]}",
            max_pages=20,
            max_depth=2,
        )

        async with Crawler(config) as crawler:
            stats = await crawler.crawl()

            # Collect structure data
            results[site] = {
                "pages": stats.pages_crawled,
                "structures": {},
            }

            for domain, structures in crawler.structure_store.get_all().items():
                for page_type, structure in structures.items():
                    results[site]["structures"][page_type] = {
                        "tag_count": len(structure.tag_hierarchy),
                        "css_classes": len(structure.css_class_map),
                        "has_article": "article" in structure.semantic_landmarks,
                        "has_nav": "nav" in structure.semantic_landmarks,
                    }

    # Compare results
    print("\n=== Site Comparison ===")
    for site, data in results.items():
        print(f"\n{site}:")
        print(f"  Pages crawled: {data['pages']}")
        for page_type, info in data["structures"].items():
            print(f"  {page_type}: {info['tag_count']} tags, {info['css_classes']} classes")

asyncio.run(compare_sites())
```

#### Example 6: Sitemap-Based Crawl

```python
import asyncio
from crawler.core.crawler import Crawler
from crawler.config import CrawlConfig
from crawler.compliance import SitemapFetcher

async def crawl_from_sitemap():
    """Use sitemap to discover and prioritize URLs."""
    # First, fetch URLs from sitemap
    async with SitemapFetcher(user_agent="MyCrawler/1.0") as fetcher:
        sitemap_urls = await fetcher.discover_sitemaps("example.com")

        all_urls = []
        async for sitemap in fetcher.fetch_all_sitemaps(sitemap_urls):
            for url in sitemap.urls:
                all_urls.append({
                    "url": url.loc,
                    "priority": url.priority or 0.5,
                    "lastmod": url.lastmod,
                })

        print(f"Found {len(all_urls)} URLs in sitemap")

    # Sort by priority (highest first)
    all_urls.sort(key=lambda x: x["priority"], reverse=True)

    # Crawl top priority URLs first
    seed_urls = [u["url"] for u in all_urls[:100]]

    config = CrawlConfig(
        seed_urls=seed_urls,
        output_dir="./sitemap_crawl",
        max_pages=500,
    )

    async with Crawler(config) as crawler:
        stats = await crawler.crawl()

    print(f"Crawled {stats.pages_crawled} pages from sitemap")

asyncio.run(crawl_from_sitemap())
```

#### Example 7: JavaScript-Heavy Site

```python
import asyncio
from crawler.core.crawler import Crawler
from crawler.core.renderer import JSRenderer, HybridFetcher
from crawler.config import CrawlConfig

async def crawl_spa():
    """Crawl a JavaScript-heavy Single Page Application."""
    config = CrawlConfig(
        seed_urls=["https://spa.example.com"],
        output_dir="./spa_data",
        max_pages=50,
    )

    # Configure JS rendering
    async with JSRenderer(
        browser_type="chromium",
        headless=True,
    ) as renderer:
        # Use hybrid fetcher that automatically detects JS need
        async with HybridFetcher(js_renderer=renderer) as fetcher:
            async with Crawler(config, fetcher=fetcher) as crawler:
                @crawler.on_page_crawled
                def log_render(url: str, result):
                    if result.used_js_rendering:
                        print(f"ğŸŒ JS rendered: {url}")
                    else:
                        print(f"ğŸ“„ HTTP only: {url}")

                stats = await crawler.crawl()

    print(f"Crawled {stats.pages_crawled} SPA pages")

asyncio.run(crawl_spa())
```

#### Example 8: Distributed Crawl

```python
import asyncio
from crawler.core.distributed import (
    DistributedCrawlManager,
    CrawlerWorker,
)
import redis.asyncio as redis

async def run_distributed_crawl():
    """Run a distributed crawl across multiple workers."""
    redis_client = redis.from_url("redis://localhost:6379/0")

    # Manager creates the job
    manager = DistributedCrawlManager(redis_client)

    job = await manager.create_job(
        job_id="large-crawl-001",
        seed_urls=[
            "https://example1.com",
            "https://example2.com",
            "https://example3.com",
        ],
        max_urls=10000,
        max_depth=5,
    )

    print(f"Created job: {job.job_id}")
    print(f"Seed URLs: {len(job.seed_urls)}")

    # In production, run workers on different machines
    # Here we simulate with multiple async workers
    workers = []
    for i in range(3):
        worker = CrawlerWorker(
            redis_client=redis_client,
            job_id=job.job_id,
            worker_id=f"worker-{i}",
        )
        workers.append(worker.start())

    # Monitor progress
    async def monitor():
        while True:
            status = await manager.get_job_status(job.job_id)
            print(f"\rPending: {status['pending']} | "
                  f"Processing: {status['processing']} | "
                  f"Completed: {status['completed']}", end="")

            if status['state'] == 'COMPLETED':
                break
            await asyncio.sleep(2)

    # Run workers and monitor concurrently
    await asyncio.gather(
        *workers,
        monitor(),
    )

    print(f"\nDistributed crawl complete!")

asyncio.run(run_distributed_crawl())
```

### Output Format

The crawler produces structured output in the following format:

#### Directory Structure
```
output/
â”œâ”€â”€ raw/                          # Raw HTML files
â”‚   â””â”€â”€ example.com/
â”‚       â”œâ”€â”€ index.html
â”‚       â””â”€â”€ about.html
â”œâ”€â”€ extracted/                    # Extracted JSON data
â”‚   â””â”€â”€ example.com/
â”‚       â”œâ”€â”€ index.json
â”‚       â””â”€â”€ about.json
â”œâ”€â”€ structures/                   # Learned page structures
â”‚   â””â”€â”€ example.com/
â”‚       â”œâ”€â”€ homepage.json
â”‚       â””â”€â”€ article.json
â”œâ”€â”€ metadata/
â”‚   â”œâ”€â”€ crawl_stats.json         # Overall statistics
â”‚   â”œâ”€â”€ url_map.json             # URL to file mapping
â”‚   â””â”€â”€ errors.json              # Error log
â””â”€â”€ logs/
    â””â”€â”€ crawler.log              # Full crawl log
```

#### Extracted JSON Format
```json
{
    "url": "https://example.com/article/123",
    "crawled_at": "2025-01-30T10:30:00Z",
    "status_code": 200,
    "content_type": "text/html",
    "content_length": 15234,
    "extracted": {
        "title": "Article Title Here",
        "content": "Full article text content...",
        "description": "Meta description if available",
        "author": "John Doe",
        "published_date": "2025-01-29",
        "images": [
            "https://example.com/img/hero.jpg"
        ],
        "links": [
            {"href": "/related/456", "text": "Related Article"}
        ]
    },
    "structure": {
        "page_type": "article",
        "similarity_to_stored": 0.97,
        "version": 3
    },
    "compliance": {
        "robots_allowed": true,
        "crawl_delay_respected": true,
        "pii_detected": false
    }
}
```

---

## Sports News Monitor Example

The `examples/sports_news_monitor.py` demonstrates real-world usage for monitoring websites for changes.

### Use Case

Monitor sports news sites (ESPN, BBC Sport, etc.) for content updates:
- Learn page structure on first visit
- Detect structural changes (site redesigns)
- Extract content when changes occur
- Ignore dynamic content (timestamps, scores)

### How It Works

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    MONITORING WORKFLOW                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                  â”‚
â”‚  1. FETCH PAGE                                                   â”‚
â”‚     â””â”€â”€â–º HTTP GET with robots.txt respect                       â”‚
â”‚                                                                  â”‚
â”‚  2. ANALYZE STRUCTURE                                            â”‚
â”‚     â””â”€â”€â–º Create DOM fingerprint (tags, classes, IDs)            â”‚
â”‚                                                                  â”‚
â”‚  3. COMPUTE STRUCTURAL FINGERPRINT                               â”‚
â”‚     â””â”€â”€â–º Hash of structural elements only                       â”‚
â”‚     â””â”€â”€â–º Ignores: timestamps, scores, text content              â”‚
â”‚                                                                  â”‚
â”‚  4. COMPARE FINGERPRINTS                                         â”‚
â”‚     â”œâ”€â”€â–º Same fingerprint â”€â”€â–º No changes, skip extraction       â”‚
â”‚     â””â”€â”€â–º Different fingerprint â”€â”€â–º Continue to step 5           â”‚
â”‚                                                                  â”‚
â”‚  5. DETECT CHANGE TYPE                                           â”‚
â”‚     â”œâ”€â”€â–º First visit â”€â”€â–º "new_content"                          â”‚
â”‚     â”œâ”€â”€â–º < 70% similar â”€â”€â–º "structure_changed" (adapt)          â”‚
â”‚     â””â”€â”€â–º â‰¥ 70% similar â”€â”€â–º "content_updated"                    â”‚
â”‚                                                                  â”‚
â”‚  6. EXTRACT CONTENT                                              â”‚
â”‚     â””â”€â”€â–º Apply learned CSS selectors                            â”‚
â”‚     â””â”€â”€â–º Get title, content, metadata                           â”‚
â”‚                                                                  â”‚
â”‚  7. SAVE & NOTIFY                                                â”‚
â”‚     â””â”€â”€â–º Save to JSON file                                      â”‚
â”‚     â””â”€â”€â–º Trigger callback                                       â”‚
â”‚                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Structural Fingerprinting

The monitor uses structural fingerprints to avoid false positives:

```python
# What's INCLUDED in fingerprint (structural):
- Tag counts (div: 45, span: 23, etc.)
- CSS class names (top 30 most frequent)
- Element IDs
- Semantic landmarks
- Navigation selectors

# What's EXCLUDED (dynamic content):
- Text content
- Timestamps and dates
- Scores and statistics
- Image URLs
- Ad content
```

### Usage

```bash
# Install Redis first (if not running)
python examples/sports_news_monitor.py --install-redis

# Monitor a URL (one-time check)
python examples/sports_news_monitor.py \
    --url https://news.ycombinator.com \
    --once

# Continuous monitoring
python examples/sports_news_monitor.py \
    --url https://www.bbc.com/sport \
    --interval 300 \
    --output ./sports_output

# Multiple URLs
python examples/sports_news_monitor.py \
    --url https://www.espn.com \
    --url https://www.espn.com/nfl \
    --interval 600
```

### Output

Changes are saved to JSON:

```json
{
    "url": "https://www.espn.com/nfl/",
    "detected_at": "2025-01-29T10:30:00Z",
    "change_type": "content_updated",
    "similarity_score": 0.97,
    "previous_hash": "a1b2c3d4e5f6g7h8",
    "current_hash": "h8g7f6e5d4c3b2a1",
    "extracted": {
        "title": "NFL News - Latest Headlines",
        "content_preview": "Breaking: Team announces...",
        "content_length": 4523,
        "metadata": {},
        "images": ["https://..."]
    }
}
```

---

## API Reference

### Core Classes

#### `Crawler`

Main orchestrator for crawling operations.

```python
class Crawler:
    def __init__(
        self,
        config: CrawlConfig,
        redis_url: str = "redis://localhost:6379/0",
        user_agent: str = "AdaptiveCrawler/1.0",
    ): ...

    async def start(self) -> None:
        """Initialize all components."""

    async def stop(self) -> None:
        """Cleanup and close connections."""

    async def crawl(self) -> CrawlerStats:
        """Run the crawl and return statistics."""

    def on_page_crawled(self, callback: Callable) -> None:
        """Register callback for successful crawls."""

    def on_error(self, callback: Callable) -> None:
        """Register callback for errors."""
```

#### `StructureAnalyzer`

Analyzes HTML to create page structure fingerprints.

```python
class StructureAnalyzer:
    def analyze(
        self,
        html: str,
        url: str,
        page_type: str = "unknown",
    ) -> PageStructure:
        """Analyze HTML and return structure fingerprint."""
```

#### `ChangeDetector`

Detects and classifies changes between structures.

```python
class ChangeDetector:
    def detect_changes(
        self,
        old_structure: PageStructure,
        new_structure: PageStructure,
    ) -> ChangeAnalysis:
        """Compare structures and return analysis."""

    def has_breaking_changes(
        self,
        old_structure: PageStructure,
        new_structure: PageStructure,
    ) -> bool:
        """Quick check for breaking changes."""
```

#### `StrategyLearner`

Learns CSS selectors for content extraction.

```python
class StrategyLearner:
    def infer(
        self,
        html: str,
        structure: PageStructure | None = None,
    ) -> LearnedStrategy:
        """Infer extraction strategy from HTML."""

    def adapt(
        self,
        old_strategy: ExtractionStrategy,
        new_structure: PageStructure,
        html: str,
    ) -> LearnedStrategy:
        """Adapt existing strategy to new structure."""
```

#### `ContentExtractor`

Extracts content using learned strategies.

```python
class ContentExtractor:
    def extract(
        self,
        url: str,
        html: str,
        strategy: ExtractionStrategy,
    ) -> ExtractionResult:
        """Extract content using strategy."""
```

### Data Models

#### `PageStructure`

```python
@dataclass
class PageStructure:
    domain: str
    page_type: str
    url_pattern: str
    tag_hierarchy: dict[str, Any]
    css_class_map: dict[str, int]
    id_attributes: set[str]
    semantic_landmarks: dict[str, list[str]]
    content_regions: list[ContentRegion]
    navigation_selectors: list[str]
    content_hash: str
    version: int = 1
```

#### `ExtractionStrategy`

```python
@dataclass
class ExtractionStrategy:
    domain: str
    page_type: str
    title: SelectorRule | None
    content: SelectorRule | None
    metadata: dict[str, SelectorRule]
    confidence_scores: dict[str, float]
    required_fields: list[str] = ["title", "content"]
    version: int = 1
```

#### `ChangeAnalysis`

```python
@dataclass
class ChangeAnalysis:
    has_changes: bool
    classification: ChangeClassification  # COSMETIC, MINOR, MODERATE, BREAKING
    similarity_score: float
    changes: list[StructureChange]
    requires_relearning: bool
```

---

## Machine Learning Features

The crawler includes advanced ML capabilities for semantic change detection, content classification, and LLM-powered descriptions.

### ML Architecture Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    ML-ENHANCED PIPELINE                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                  â”‚
â”‚  PAGE STRUCTURE                                                  â”‚
â”‚       â”‚                                                          â”‚
â”‚       â–¼                                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚              EMBEDDING MODEL                             â”‚    â”‚
â”‚  â”‚         (sentence-transformers)                          â”‚    â”‚
â”‚  â”‚                                                          â”‚    â”‚
â”‚  â”‚  â€¢ all-MiniLM-L6-v2 (default, 384 dims, fast)           â”‚    â”‚
â”‚  â”‚  â€¢ all-mpnet-base-v2 (768 dims, best quality)           â”‚    â”‚
â”‚  â”‚  â€¢ paraphrase-MiniLM-L6-v2 (paraphrase optimized)       â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚       â”‚                                                          â”‚
â”‚       â–¼                                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚           ML CAPABILITIES                                â”‚    â”‚
â”‚  â”‚                                                          â”‚    â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚    â”‚
â”‚  â”‚  â”‚  Semantic   â”‚  â”‚    Page     â”‚  â”‚    LLM      â”‚     â”‚    â”‚
â”‚  â”‚  â”‚  Similarity â”‚  â”‚   Type      â”‚  â”‚ Description â”‚     â”‚    â”‚
â”‚  â”‚  â”‚  Detection  â”‚  â”‚ Classifier  â”‚  â”‚  Generator  â”‚     â”‚    â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚    â”‚
â”‚  â”‚        â”‚                â”‚                â”‚              â”‚    â”‚
â”‚  â”‚        â–¼                â–¼                â–¼              â”‚    â”‚
â”‚  â”‚   cosine sim       LR/XGB/LGBM    OpenAI/Anthropic     â”‚    â”‚
â”‚  â”‚   threshold        prediction      /Ollama             â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Enabling ML Features

#### 1. Install ML Dependencies

```bash
# Core ML (embeddings + classification)
pip install sentence-transformers scikit-learn

# Gradient boosting classifiers
pip install xgboost lightgbm

# LLM providers
pip install openai anthropic

# All features
pip install -e ".[ml,llm]"
```

#### 2. Configure ML in Python

```python
from crawler.config import (
    CrawlConfig,
    StructureStoreConfig,
    StructureStoreType,
    LLMProviderType,
)

config = CrawlConfig(
    seed_urls=["https://example.com"],
    output_dir="./data",

    structure_store=StructureStoreConfig(
        # Enable LLM-powered descriptions
        store_type=StructureStoreType.LLM,

        # Enable embeddings for semantic similarity
        enable_embeddings=True,
        embedding_model="all-MiniLM-L6-v2",

        # LLM provider settings
        llm_provider=LLMProviderType.ANTHROPIC,
        llm_model="claude-sonnet-4-20250514",
        # API key from environment: ANTHROPIC_API_KEY
    ),
)
```

#### 3. Environment Variables

```bash
# LLM API Keys
export OPENAI_API_KEY="sk-..."
export ANTHROPIC_API_KEY="sk-ant-..."
export OLLAMA_API_KEY="your-key"  # For Ollama Cloud

# Local Ollama (no key needed)
# Just run: ollama serve
```

### Embedding-Based Change Detection

Use semantic embeddings instead of rule-based comparison for more intelligent change detection.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                EMBEDDING SIMILARITY DETECTION                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                  â”‚
â”‚  OLD STRUCTURE                    NEW STRUCTURE                  â”‚
â”‚       â”‚                                â”‚                         â”‚
â”‚       â–¼                                â–¼                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”‚
â”‚  â”‚  Embedding  â”‚                â”‚  Embedding  â”‚                 â”‚
â”‚  â”‚  [384 dims] â”‚                â”‚  [384 dims] â”‚                 â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜                â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜                 â”‚
â”‚         â”‚                              â”‚                         â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                         â”‚
â”‚                        â–¼                                         â”‚
â”‚               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                â”‚
â”‚               â”‚ Cosine Similarityâ”‚                               â”‚
â”‚               â”‚   (0.0 - 1.0)   â”‚                                â”‚
â”‚               â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                â”‚
â”‚                        â”‚                                         â”‚
â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                         â”‚
â”‚         â”‚              â”‚              â”‚                          â”‚
â”‚         â–¼              â–¼              â–¼                          â”‚
â”‚     â‰¥ 0.95         0.7-0.95        < 0.70                       â”‚
â”‚    COSMETIC       MODERATE        BREAKING                       â”‚
â”‚   (no action)   (log change)    (re-learn)                      â”‚
â”‚                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### Python API

```python
from crawler.ml.embeddings import StructureEmbeddingModel, MLChangeDetector

# Create embedding model
model = StructureEmbeddingModel(model_name="all-MiniLM-L6-v2")

# Embed structures
old_embedding = model.embed_structure(old_structure)
new_embedding = model.embed_structure(new_structure)

# Compute similarity
similarity = model.compute_similarity(
    old_embedding.embedding,
    new_embedding.embedding
)
print(f"Semantic similarity: {similarity:.2%}")

# Find similar structures
similar = model.find_similar(
    query_embedding.embedding,
    all_embeddings,
    top_k=5
)

# ML-based change detection
detector = MLChangeDetector(embedding_model=model)
result = detector.detect_change(old_structure, new_structure)
print(f"Is breaking: {result['is_breaking']}")
print(f"Similarity: {result['similarity']:.2%}")
```

### Page Type Classification

Train ML classifiers to automatically categorize pages.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 PAGE TYPE CLASSIFICATION                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                  â”‚
â”‚  INPUT: Page Structure                                           â”‚
â”‚       â”‚                                                          â”‚
â”‚       â–¼                                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚              FEATURE EXTRACTION                          â”‚    â”‚
â”‚  â”‚                                                          â”‚    â”‚
â”‚  â”‚  â€¢ Tag counts (div, article, nav, etc.)                 â”‚    â”‚
â”‚  â”‚  â€¢ CSS class patterns                                   â”‚    â”‚
â”‚  â”‚  â€¢ Semantic landmarks                                   â”‚    â”‚
â”‚  â”‚  â€¢ Content region characteristics                       â”‚    â”‚
â”‚  â”‚  â€¢ Navigation patterns                                  â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚       â”‚                                                          â”‚
â”‚       â–¼                                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚              CLASSIFIER                                  â”‚    â”‚
â”‚  â”‚                                                          â”‚    â”‚
â”‚  â”‚  Choose one:                                            â”‚    â”‚
â”‚  â”‚  â€¢ LogisticRegression (fast, interpretable)             â”‚    â”‚
â”‚  â”‚  â€¢ XGBoost (high accuracy, feature importance)          â”‚    â”‚
â”‚  â”‚  â€¢ LightGBM (fast training, large datasets)             â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚       â”‚                                                          â”‚
â”‚       â–¼                                                          â”‚
â”‚  OUTPUT: Page Type + Confidence                                  â”‚
â”‚                                                                  â”‚
â”‚  Examples:                                                       â”‚
â”‚  â€¢ "article" (92% confidence)                                   â”‚
â”‚  â€¢ "homepage" (87% confidence)                                  â”‚
â”‚  â€¢ "product_listing" (78% confidence)                           â”‚
â”‚                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### Training a Classifier

```python
from crawler.ml.embeddings import StructureClassifier, ClassifierType

# Create classifier
classifier = StructureClassifier(
    classifier_type=ClassifierType.XGBOOST  # or LIGHTGBM, LOGISTIC_REGRESSION
)

# Prepare training data
structures = [structure1, structure2, ...]  # PageStructure objects
labels = ["article", "homepage", ...]        # Page type labels

# Train
metrics = classifier.train(structures, labels)
print(f"Accuracy: {metrics['accuracy']:.2%}")
print(f"F1 Score: {metrics['f1_score']:.2%}")

# Predict
label, confidence = classifier.predict(new_structure)
print(f"Predicted: {label} ({confidence:.2%} confidence)")

# Get feature importance (XGBoost/LightGBM)
importance = classifier.get_feature_importance()
for feature, score in importance[:10]:
    print(f"  {feature}: {score:.4f}")

# Save/load model
classifier.save("page_classifier.pkl")
classifier.load("page_classifier.pkl")
```

### LLM-Powered Descriptions

Generate rich, semantic descriptions of page structures using LLMs.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 LLM DESCRIPTION GENERATION                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                  â”‚
â”‚  PAGE STRUCTURE                                                  â”‚
â”‚       â”‚                                                          â”‚
â”‚       â–¼                                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  Structure Summary                                       â”‚    â”‚
â”‚  â”‚  â€¢ 45 div, 23 span, 67 anchor tags                      â”‚    â”‚
â”‚  â”‚  â€¢ Semantic: article, nav, header, footer               â”‚    â”‚
â”‚  â”‚  â€¢ Classes: .article-content, .nav-item, .btn           â”‚    â”‚
â”‚  â”‚  â€¢ Content regions: main content, sidebar               â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚       â”‚                                                          â”‚
â”‚       â–¼                                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚              LLM PROVIDER                                â”‚    â”‚
â”‚  â”‚                                                          â”‚    â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚    â”‚
â”‚  â”‚  â”‚ OpenAI  â”‚  â”‚Anthropicâ”‚  â”‚ Ollama  â”‚  â”‚ Ollama  â”‚   â”‚    â”‚
â”‚  â”‚  â”‚ GPT-4o  â”‚  â”‚ Claude  â”‚  â”‚ (Local) â”‚  â”‚ (Cloud) â”‚   â”‚    â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚       â”‚                                                          â”‚
â”‚       â–¼                                                          â”‚
â”‚  RICH DESCRIPTION:                                               â”‚
â”‚  "This is a news article page with a prominent header            â”‚
â”‚   containing navigation. The main content area uses an           â”‚
â”‚   <article> tag with structured sections. The page follows       â”‚
â”‚   a standard blog layout with sidebar widgets and a footer       â”‚
â”‚   containing social links and copyright information."            â”‚
â”‚                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### Using LLM Descriptions

```python
from crawler.ml.embeddings import get_description_generator

# Rules-based (no API, fast, deterministic)
rules_gen = get_description_generator("rules")
desc = rules_gen.generate(structure)

# OpenAI
openai_gen = get_description_generator(
    "llm",
    provider="openai",
    model="gpt-4o-mini"
)
desc = openai_gen.generate(structure)

# Anthropic (Claude)
claude_gen = get_description_generator(
    "llm",
    provider="anthropic",
    model="claude-sonnet-4-20250514"
)
desc = claude_gen.generate(structure)

# Local Ollama (free, private)
ollama_gen = get_description_generator(
    "llm",
    provider="ollama",
    model="llama3.2"
)
desc = ollama_gen.generate(structure)

# Generate change description
change_desc = generator.generate_for_change_detection(
    old_structure,
    new_structure
)
print(change_desc)
# "The page structure changed significantly: the navigation
#  moved from sidebar to header, article content wrapper
#  changed from .post-content to .article-body, and new
#  advertisement slots were added between paragraphs."
```

### ML Training Script

Use the built-in script for ML operations:

```bash
# Export training data from Redis
python scripts/train_embeddings.py export -o training_data.jsonl

# Create embeddings for all structures
python scripts/train_embeddings.py embed -o embeddings.json

# Find similar structures
python scripts/train_embeddings.py similar example.com --top-k 10

# Train classifiers
python scripts/train_embeddings.py train -o classifier.pkl
python scripts/train_embeddings.py train --classifier-type xgboost
python scripts/train_embeddings.py train --classifier-type lightgbm

# Predict page type
python scripts/train_embeddings.py predict example.com --classifier classifier.pkl

# Generate descriptions
python scripts/train_embeddings.py describe example.com --mode rules
python scripts/train_embeddings.py describe example.com --mode llm --provider anthropic

# Baseline drift detection
python scripts/train_embeddings.py set-baseline example.com
python scripts/train_embeddings.py detect-drift example.com
python scripts/train_embeddings.py set-all-baselines
python scripts/train_embeddings.py check-all-drift
```

### Baseline Drift Detection

Monitor sites for gradual structural drift over time.

```python
from crawler.ml.embeddings import MLChangeDetector

detector = MLChangeDetector()

# Set baseline from current structure
detector.set_site_baseline("example.com", current_structure)

# Later: check for drift
drift = detector.detect_drift_from_baseline(new_structure)
print(f"Drift detected: {drift['has_drift']}")
print(f"Drift amount: {drift['drift_score']:.2%}")
print(f"Recommendation: {drift['recommendation']}")
```

### ML Configuration Reference

| Option | Type | Default | Description |
|--------|------|---------|-------------|
| `store_type` | enum | `basic` | `basic` (rules) or `llm` (LLM-powered) |
| `enable_embeddings` | bool | `false` | Enable semantic embeddings |
| `embedding_model` | str | `all-MiniLM-L6-v2` | HuggingFace model name |
| `llm_provider` | enum | `anthropic` | `anthropic`, `openai`, `ollama`, `ollama-cloud` |
| `llm_model` | str | `""` | Model name (empty = provider default) |
| `llm_api_key` | str | `""` | API key (empty = use env var) |
| `ollama_base_url` | str | `http://localhost:11434` | Ollama server URL |

### LLM Provider Comparison

| Provider | Models | Latency | Cost | Privacy |
|----------|--------|---------|------|---------|
| **OpenAI** | gpt-4o-mini, gpt-4 | Low | $$ | Cloud |
| **Anthropic** | claude-sonnet, claude-opus | Low | $$ | Cloud |
| **Ollama (Local)** | llama3.2, mistral, codellama | Medium | Free | Full |
| **Ollama Cloud** | Same as local | Low | Varies | Depends |

### Example: Full ML Pipeline

```python
import asyncio
from crawler.core.crawler import Crawler
from crawler.config import CrawlConfig, StructureStoreConfig, StructureStoreType
from crawler.ml.embeddings import (
    StructureEmbeddingModel,
    StructureClassifier,
    MLChangeDetector,
    get_description_generator,
)

async def ml_enhanced_crawl():
    # Configure with ML features
    config = CrawlConfig(
        seed_urls=["https://example.com"],
        output_dir="./data",
        structure_store=StructureStoreConfig(
            store_type=StructureStoreType.LLM,
            enable_embeddings=True,
            embedding_model="all-MiniLM-L6-v2",
            llm_provider="anthropic",
        ),
    )

    # Initialize ML components
    embedding_model = StructureEmbeddingModel()
    classifier = StructureClassifier(classifier_type="xgboost")
    change_detector = MLChangeDetector(embedding_model=embedding_model)
    description_gen = get_description_generator("llm", provider="anthropic")

    # Run crawler
    async with Crawler(config) as crawler:
        stats = await crawler.crawl()

    # Post-process with ML
    for structure in collected_structures:
        # Classify page type
        page_type, confidence = classifier.predict(structure)
        print(f"Page type: {page_type} ({confidence:.0%})")

        # Generate description
        desc = description_gen.generate(structure)
        print(f"Description: {desc}")

        # Check similarity to baseline
        drift = change_detector.detect_drift_from_baseline(structure)
        if drift['has_drift']:
            print(f"Warning: Site structure drifted {drift['drift_score']:.0%}")

asyncio.run(ml_enhanced_crawl())
```

---

## Development

### Running Tests

```bash
# All tests with coverage
pytest tests/ -v --cov=crawler

# Specific test file
pytest tests/unit/test_change_detector.py -v

# With parallel execution
pytest tests/ -v -n auto
```

### Type Checking

```bash
mypy crawler/ --strict
```

### Linting & Formatting

```bash
# Check and fix
ruff check crawler/ --fix

# Format
ruff format crawler/
```

### Coverage Requirements

| Module | Minimum |
|--------|---------|
| `compliance/*` | 100% |
| `legal/*` | 100% |
| `adaptive/*` | 95% |
| `core/*` | 90% |
| **Overall** | **90%** |

---

## Documentation

- [AGENTS.md](AGENTS.md) - Comprehensive project documentation
- [crawler/adaptive/AGENTS.md](crawler/adaptive/AGENTS.md) - Adaptive subsystem details
- [CLAUDE.md](CLAUDE.md) - Claude Code development guidance

---

## Legal Notice

This crawler is designed for **ethical, legal web data collection**. This section provides guidance on legal compliance, but **this is technical documentation, not legal advice**. Always consult with a qualified attorney for your specific use case.

### Your Legal Responsibilities

As a user of this crawler, you are responsible for:

| Responsibility | Description |
|----------------|-------------|
| **Legal Compliance** | Comply with all applicable laws including CFAA, GDPR, CCPA, and local regulations |
| **Terms of Service** | Respect website terms of service and acceptable use policies |
| **Authorization** | Ensure you have proper authorization before crawling any site |
| **Rate Limiting** | Configure appropriate rate limits to avoid service disruption |
| **Data Handling** | Use collected data responsibly and in accordance with privacy laws |
| **Legal Counsel** | Obtain legal advice for your specific jurisdiction and use case |

### Legal Frameworks

#### Computer Fraud and Abuse Act (CFAA) - United States

The CFAA prohibits unauthorized access to computer systems. Key considerations:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    CFAA COMPLIANCE CHECKLIST                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                  â”‚
â”‚  âœ“ Public Content Only                                          â”‚
â”‚    Only crawl publicly accessible pages (no login required)     â”‚
â”‚                                                                  â”‚
â”‚  âœ“ Respect Access Controls                                      â”‚
â”‚    Stop if you encounter authentication prompts                 â”‚
â”‚                                                                  â”‚
â”‚  âœ“ Honor robots.txt                                             â”‚
â”‚    Respect crawl restrictions and rate limits                   â”‚
â”‚                                                                  â”‚
â”‚  âœ“ Identify Your Bot                                            â”‚
â”‚    Use a clear User-Agent with contact information              â”‚
â”‚                                                                  â”‚
â”‚  âœ“ Stop on Request                                              â”‚
â”‚    Immediately cease crawling if asked by site owner            â”‚
â”‚                                                                  â”‚
â”‚  âœ“ No Circumvention                                             â”‚
â”‚    Never bypass security measures, CAPTCHAs, or IP blocks       â”‚
â”‚                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Crawler implementation:**
```python
from crawler.legal import CFAAChecker

# The crawler automatically checks authorization
checker = CFAAChecker()
result = await checker.is_authorized(url)

# Blocks crawling if:
# - Authentication is required
# - Site has sent cease-and-desist
# - robots.txt explicitly blocks crawlers
# - Previous access was denied
```

#### General Data Protection Regulation (GDPR) - European Union

GDPR applies when collecting data from EU residents. Requirements:

| Principle | Implementation |
|-----------|----------------|
| **Lawful Basis** | Ensure legitimate interest or consent for data collection |
| **Data Minimization** | Only collect data necessary for your stated purpose |
| **Purpose Limitation** | Don't use collected data for incompatible purposes |
| **Storage Limitation** | Delete data when no longer needed (configure retention) |
| **Accuracy** | Keep collected data accurate and up-to-date |
| **Security** | Implement appropriate security measures |
| **Rights** | Support data subject rights (access, deletion, etc.) |

**Crawler implementation:**
```python
from crawler.config import GDPRConfig, PIIHandlingConfig

config = CrawlConfig(
    gdpr=GDPRConfig(
        enabled=True,
        retention_days=365,           # Auto-delete after 1 year
        collect_only=["url", "title", "content"],  # Data minimization
    ),
    pii=PIIHandlingConfig(
        action="redact",              # Remove PII from collected data
        log_detections=True,          # Audit trail
    ),
)
```

#### California Consumer Privacy Act (CCPA)

CCPA provides California residents with privacy rights. Key considerations:

- Right to know what data is collected
- Right to delete personal information
- Right to opt-out of data sale
- Non-discrimination for exercising rights

**Best practices:**
- Document what data you collect and why
- Implement data deletion capabilities
- Never sell collected personal data without consent
- Maintain records of data collection activities

### Ethical Crawling Guidelines

Beyond legal requirements, follow these ethical guidelines:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    ETHICAL CRAWLING PRINCIPLES                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                  â”‚
â”‚  1. TRANSPARENCY                                                 â”‚
â”‚     â€¢ Identify your crawler clearly in User-Agent               â”‚
â”‚     â€¢ Provide contact information                               â”‚
â”‚     â€¢ Explain your purpose if asked                             â”‚
â”‚                                                                  â”‚
â”‚  2. RESPECT                                                      â”‚
â”‚     â€¢ Honor robots.txt directives                               â”‚
â”‚     â€¢ Respect Crawl-delay specifications                        â”‚
â”‚     â€¢ Stop crawling if asked                                    â”‚
â”‚                                                                  â”‚
â”‚  3. MINIMAL IMPACT                                               â”‚
â”‚     â€¢ Use appropriate rate limiting                             â”‚
â”‚     â€¢ Avoid peak traffic hours for high-volume crawls           â”‚
â”‚     â€¢ Don't overwhelm small servers                             â”‚
â”‚                                                                  â”‚
â”‚  4. DATA RESPONSIBILITY                                          â”‚
â”‚     â€¢ Only collect what you need                                â”‚
â”‚     â€¢ Store data securely                                       â”‚
â”‚     â€¢ Delete data when no longer needed                         â”‚
â”‚     â€¢ Never collect or store PII unnecessarily                  â”‚
â”‚                                                                  â”‚
â”‚  5. GOOD CITIZENSHIP                                             â”‚
â”‚     â€¢ Don't crawl content behind paywalls                       â”‚
â”‚     â€¢ Respect copyright and intellectual property               â”‚
â”‚     â€¢ Don't redistribute collected content without permission   â”‚
â”‚                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### User-Agent Best Practices

Always identify your crawler with a proper User-Agent:

```python
# Good User-Agent examples:
user_agent = "CompanyBot/1.0 (+https://company.com/bot; bot@company.com)"
user_agent = "ResearchCrawler/2.0 (+https://university.edu/research-bot; researcher@university.edu)"
user_agent = "NewsAggregator/1.5 (https://news-site.com/about; contact@news-site.com)"

# Include:
# - Bot name and version
# - URL with more information
# - Contact email for issues

# Bad User-Agent examples (don't do this):
# - "" (empty)
# - "Mozilla/5.0" (pretending to be a browser)
# - "curl/7.68.0" (anonymous)
```

### Handling Blocks and Restrictions

When a site blocks or restricts your crawler:

1. **Respect the block immediately** - Don't try to circumvent
2. **Review your crawling behavior** - Were you too aggressive?
3. **Contact the site owner** - Explain your purpose, ask for permission
4. **Wait before retrying** - Give adequate time before checking again
5. **Document the block** - Keep records for compliance purposes

```python
# The crawler automatically handles blocks
from crawler.compliance import BlockedDomainTracker

tracker = BlockedDomainTracker(redis_client)

# Check before crawling
if await tracker.is_blocked("example.com"):
    # Don't crawl - the site has blocked us
    reason = await tracker.get_block_reason("example.com")
    blocked_since = await tracker.get_block_time("example.com")
    # Consider contacting site owner

# Blocks are recorded automatically when:
# - Receiving 403 Forbidden responses
# - Encountering CAPTCHA challenges
# - Getting IP-blocked
# - Receiving cease-and-desist requests
```

### Disclaimer

**IMPORTANT NOTICES:**

1. **Not Legal Advice**: This documentation provides technical guidance only. It is not legal advice and should not be relied upon as such.

2. **User Responsibility**: Users are solely responsible for ensuring their use of this crawler complies with all applicable laws, regulations, and third-party terms of service.

3. **No Warranties**: This software is provided "as is" without warranties of any kind. The authors are not liable for any damages or legal issues arising from its use.

4. **Jurisdiction Varies**: Laws regarding web scraping vary significantly by jurisdiction. What's legal in one country may be illegal in another.

5. **Consult an Attorney**: Before using this crawler for any commercial purpose or on any scale, consult with a qualified attorney familiar with technology law in your jurisdiction.

**By using this crawler, you acknowledge that you have read and understood these disclaimers and accept full responsibility for your use of the software.**

---

## License

MIT License - See [LICENSE](LICENSE) for details.

---

## Contributing

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Make your changes with tests
4. Ensure all tests pass (`pytest tests/ -v`)
5. Submit a pull request

See [CONTRIBUTING.md](CONTRIBUTING.md) for detailed guidelines.
